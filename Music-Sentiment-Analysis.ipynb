{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [Errno 11001] getaddrinfo failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading sentiwordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading vader_lexicon: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "# %load ImportsDefinitions.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "#Imports\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import joblib \n",
    "import pickle\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm, tree\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "#accuracy_score(labels_test,pred)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#accuracy_score(labels_test,pred)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "from yellowbrick.datasets import load_hobbies\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.datasets import load_occupancy\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "from yellowbrick.datasets import load_hobbies\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "#Ορισμός συναρτήσεων για να ελέξγχουμε τον καλύτερο vectorizer\n",
    "def tfidf_test_simple(X_train,X_test,y_train,y_test,token_izer):\n",
    "    if (token_izer=='1'):\n",
    "        tfvect= TfidfVectorizer(tokenizer=tokenizer_preproccessor)\n",
    "    elif (token_izer=='2'):\n",
    "        tfvect= TfidfVectorizer(tokenizer=tokenizer_preproccessor_imdb)\n",
    "    else:\n",
    "        tfvect= TfidfVectorizer()\n",
    "    tfidf_train = tfvect.fit_transform(X_train)\n",
    "    tfidf_test = tfvect.transform(X_test)\n",
    "    nb=MultinomialNB()\n",
    "    #train the model and timing it\n",
    "    #kanoume prediction gia to x_test_dtm\n",
    "    \n",
    "    # cross val score/ predict\n",
    "    cvec_score = cross_val_score(nb, tfidf_train, y_train, cv=4 )\n",
    "    feature_names = tfvect.get_feature_names()\n",
    "    print(\"Number of features: {}\".format(len(feature_names)))\n",
    "    print(\"to accuracy tou TFIDF me NB einai {}\".format(cvec_score.mean()))\n",
    "    \n",
    "    visualizer = FreqDistVisualizer(features=feature_names, orient='h')\n",
    "    visualizer.fit(tfidf_train)\n",
    "    visualizer.poof()\n",
    "    \n",
    "    return cvec_score.mean()\n",
    "\n",
    "def countvect_test_simple(X_train,X_test,y_train,y_test,token_izer):\n",
    "    if (token_izer=='1'):\n",
    "        countvect= CountVectorizer(tokenizer=tokenizer_preproccessor)\n",
    "    elif (token_izer=='2'):\n",
    "        countvect= CountVectorizer(tokenizer=tokenizer_preproccessor_imdb)\n",
    "    else:\n",
    "        countvect= CountVectorizer()\n",
    "    #CountVect\n",
    "    countvect.fit(X_train)\n",
    "    #to metatrepoume se dtm sparse matrix\n",
    "    X_train_dtm=countvect.transform(X_train)\n",
    "    X_test_dtm=countvect.transform(X_test)\n",
    "    #Ftiaxnoume Multinomial Naive Bayes modelo\n",
    "    nb=MultinomialNB()\n",
    "    #kanoume prediction gia to x_test_dtm\n",
    "    \n",
    "    # cross val score/ predict\n",
    "    cvec_score = cross_val_score(nb, X_train_dtm, y_train, cv=4 )\n",
    "        \n",
    "          \n",
    "    feature_names = countvect.get_feature_names()\n",
    "    print(\"Number of features: {}\".format(len(feature_names)))\n",
    "    print(\"to accuracy tou CountVectorizer me NB einai: {}\".format(cvec_score.mean()))\n",
    "    \n",
    "    visualizer = FreqDistVisualizer(features=feature_names, orient='h')\n",
    "    visualizer.fit(X_train_dtm)\n",
    "    visualizer.poof()\n",
    "    \n",
    "    return cvec_score.mean()\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "#oρισμός συναρτήσεων για να ελέγχουμε το max_df\n",
    "def countvect_test_maxdf(X_train,X_test,y_train,y_test,token_izer,maxdf):\n",
    "    if (token_izer=='1'):\n",
    "        countvect= CountVectorizer(tokenizer=tokenizer_preproccessor,max_df=maxdf)\n",
    "    elif (token_izer=='2'):\n",
    "        countvect= CountVectorizer(tokenizer=tokenizer_preproccessor_imdb,max_df=maxdf)\n",
    "    else:\n",
    "        countvect= CountVectorizer(max_df=maxdf)\n",
    "    X_train_dtm = countvect.fit_transform(X_train)\n",
    "    X_test_dtm = countvect.transform(X_test)\n",
    "    nb=MultinomialNB()\n",
    "    cvec_score = cross_val_score(nb, X_train_dtm, y_train, cv=4 )\n",
    "   \n",
    "    #kanoume evaluate ta apotelesmata mas me Logistic Regression\n",
    "    feature_names = countvect.get_feature_names()\n",
    "    print(\"Number of features: {}\".format(len(feature_names)))\n",
    "    print(\"To accuracy NB me Max df: {} είναι : {} \".format(maxdf,cvec_score))\n",
    "    return cvec_score.mean()\n",
    "\n",
    "\n",
    "def tfidf_test_maxdf(X_train,X_test,y_train,y_test,token_izer,maxdf):\n",
    "    if (token_izer=='1'):\n",
    "        tfvect= TfidfVectorizer(tokenizer=tokenizer_preproccessor,max_df=maxdf)\n",
    "    elif (token_izer=='2'):\n",
    "        tfvect= TfidfVectorizer(tokenizer=tokenizer_preproccessor_imdb,max_df=maxdf)\n",
    "    else:\n",
    "        tfvect= TfidfVectorizer(max_df=maxdf)\n",
    "    tfidf_train = tfvect.fit_transform(X_train)\n",
    "    tfidf_test = tfvect.transform(X_test)\n",
    "    nb=MultinomialNB()\n",
    "    cvec_score = cross_val_score(nb, tfidf_train, y_train, cv=4 )\n",
    "    \n",
    "    #kanoume evaluate ta apotelesmata mas me Logistic Regression\n",
    "    feature_names = tfvect.get_feature_names()\n",
    "    print(\"Number of features: {}\".format(len(feature_names)))\n",
    "    print(\"To accuracy NB me max df: {} είναι : {} \".format(maxdf,cvec_score))\n",
    "    return cvec_score.mean()\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "def countvect_test_ngrams(X_train,X_test,y_train,y_test,token_izer,ngrams):\n",
    "    if (token_izer=='1'):\n",
    "        countvect= CountVectorizer(tokenizer=tokenizer_preproccessor,ngram_range=(1,ngrams))\n",
    "    elif (token_izer=='2'):\n",
    "        countvect= CountVectorizer(tokenizer=tokenizer_preproccessor_imdb,ngram_range=(1,ngrams))\n",
    "    else:\n",
    "        countvect= CountVectorizer(ngram_range=(1,ngrams))\n",
    "    X_train_dtm = countvect.fit_transform(X_train)\n",
    "    X_test_dtm = countvect.transform(X_test)\n",
    "    nb=MultinomialNB()\n",
    "    cvec_score = cross_val_score(nb, X_train_dtm, y_train, cv=4 )\n",
    "   \n",
    "    #kanoume evaluate ta apotelesmata mas me Logistic Regression\n",
    "    feature_names = countvect.get_feature_names()\n",
    "    print(\"Number of features: {}\".format(len(feature_names)))\n",
    "    print(\"To accuracy NB me {}-ngrams είναι : {} \".format(ngrams,cvec_score))\n",
    "    return cvec_score.mean()\n",
    "\n",
    "def tfidf_test_ngrams(X_train,X_test,y_train,y_test,token_izer,ngrams):\n",
    "    if (token_izer=='1'):\n",
    "        tfvect= TfidfVectorizer(tokenizer=tokenizer_preproccessor,ngram_range=(1,ngrams))\n",
    "    elif (token_izer=='2'):\n",
    "        tfvect= TfidfVectorizer(tokenizer=tokenizer_preproccessor_imdb,ngram_range=(1,ngrams))\n",
    "    else:\n",
    "        tfvect= TfidfVectorizer(ngram_range=(1,ngrams))\n",
    "    tfidf_train = tfvect.fit_transform(X_train)\n",
    "    tfidf_test = tfvect.transform(X_test)\n",
    "    nb=MultinomialNB()\n",
    "    cvec_score = cross_val_score(nb, tfidf_train, y_train, cv=4 )\n",
    "    \n",
    "    #kanoume evaluate ta apotelesmata mas me Logistic Regression\n",
    "    feature_names = tfvect.get_feature_names()\n",
    "    print(\"Number of features: {}\".format(len(feature_names)))\n",
    "    print(\"To accuracy NB me {}-ngrams είναι : {} \".format(ngrams,cvec_score))\n",
    "    return cvec_score.mean()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def countvect_test_maxfeat(X_train,X_test,y_train,y_test,token_izer,maxfeat):  \n",
    "    if (token_izer=='1'):\n",
    "        countvect= CountVectorizer(tokenizer=tokenizer_preproccessor,max_features=maxfeat)\n",
    "    elif (token_izer=='2'):\n",
    "        countvect= CountVectorizer(tokenizer=tokenizer_preproccessor_imdb,max_features=maxfeat)\n",
    "    else:\n",
    "        countvect= CountVectorizer(max_features=maxfeat)\n",
    "    X_train_dtm = countvect.fit_transform(X_train)\n",
    "    X_test_dtm = countvect.transform(X_test)\n",
    "    nb=MultinomialNB()\n",
    "    cvec_score = cross_val_score(nb, X_train_dtm, y_train, cv=4 )\n",
    "   \n",
    "    #kanoume evaluate ta apotelesmata mas me Logistic Regression\n",
    "    feature_names = countvect.get_feature_names()\n",
    "    print(\"Number of features: {}\".format(len(feature_names)))\n",
    "    print(\"To accuracy NB me max features {} είναι : {} \".format(maxfeat,cvec_score))\n",
    "    return cvec_score.mean()\n",
    "\n",
    "def tfidf_test_maxfeat(X_train,X_test,y_train,y_test,token_izer,maxfeat):\n",
    "    if (token_izer=='1'):\n",
    "        tfvect= TfidfVectorizer(tokenizer=tokenizer_preproccessor,max_features=maxfeat)\n",
    "    elif (token_izer=='2'):\n",
    "        tfvect= TfidfVectorizer(tokenizer=tokenizer_preproccessor_imdb,max_features=maxfeat)\n",
    "    else:\n",
    "        tfvect= TfidfVectorizer(max_features=maxfeat)\n",
    "    tfidf_train = tfvect.fit_transform(X_train)\n",
    "    tfidf_test = tfvect.transform(X_test)\n",
    "    nb=MultinomialNB()\n",
    "    cvec_score = cross_val_score(nb, tfidf_train, y_train, cv=4 )\n",
    "    \n",
    "    #kanoume evaluate ta apotelesmata mas me Logistic Regression\n",
    "    feature_names = tfvect.get_feature_names()\n",
    "    print(\"Number of features: {}\".format(len(feature_names)))\n",
    "    print(\"To accuracy NB me max featurues : {} είναι : {} \".format(maxfeat,cvec_score))\n",
    "    return cvec_score.mean()\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "def tokenizer_preproccessor(text):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    #kanoume preprocessing ta dedomena mas\n",
    "    word_tokens=word_tokenize(text.lower()) #kanoume tokenize\n",
    "    #print(\"Έχουμε \" + str(len(word_tokens))+ \" tokens\")\n",
    "    #filtered_word_tokens = [word for word in word_tokens if word not in stop] #svinoume ta stopwords\n",
    "    #print(\"Αφού αφαιρέσαμε τα stopwords, έχουμε τελικά \" + str(len(filtered_word_tokens)) + \" tokens\")\n",
    "    #print(filtered_word_tokens)\n",
    "    filtered_word_tokens = [re.sub(r'[^A-Za-z0-9]+', '', x) for x in word_tokens]  #svinoume ta punctuations\n",
    "    #filtered_word_tokens = [x for x in filtered_word_tokens if x not in ['film','movie']]  #svinoume ta punctuations    \n",
    "    filtered_word_tokens = [word for word in filtered_word_tokens if len(word)>1] #svinoume tis mikres lekseis\n",
    "    #print(\"Αφού αφαιρέσαμε τα σημεία στίξης και τις μικρές λέξεις, έχουμε τελικά \" + str(len(filtered_word_tokens)) + \" tokens\")\n",
    "    #print(filtered_word_tokens)\n",
    "    tagged_filtered_tokens=nltk.pos_tag(filtered_word_tokens) #kanoume pos tag tis lekseis gia syntaktiki analysi\n",
    "    #print(tagged_filtered_tokens)\n",
    "            #ftiaxnoume ta pos tags wste na mpoun san input sto sentisynset\n",
    "    #epeidh ta dedomena tou postag ginontai tuples, ta metatrepoume se lista\n",
    "    newtags=[] #lista me ta nea tags kai idio counter me ta tagged words\n",
    "    for tag in tagged_filtered_tokens:\n",
    "        if tag[1] in set(['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']):\n",
    "            newtags.append('v')\n",
    "        elif tag[1] in set(['JJ', 'JJR', 'JJS']):\n",
    "             newtags.append('a')\n",
    "        elif tag[1] in set(['RB', 'RBR', 'RBS']):\n",
    "             newtags.append('r')\n",
    "        elif tag[1] in set(['NNS', 'NN', 'NNP', 'NNPS']):\n",
    "             newtags.append('n')\n",
    "        else:\n",
    "             newtags.append('a')\n",
    "    \n",
    "    lem_words=[] #edw tha mpoun oi lematized lekseis pou exoume kratisei apo to preprocessing\n",
    "    counter=0 #vazoume ton counter gia na kanoume iterate ta stoixeia tis listas twn tags\n",
    "    for word in tagged_filtered_tokens:    \n",
    "        lem_words.append(wnl.lemmatize(word[0],newtags[counter]))\n",
    "        counter+=1\n",
    "    lem_words = [word for word in lem_words if word not in stop] #svinoume ta stopwords\n",
    "    return lem_words\n",
    "\n",
    "def tokenizer_preproccessor_imdb(text):\n",
    "    stop = nltk.corpus.stopwords.words('english')\n",
    "    stop.append('film')\n",
    "    stop.append('movie')\n",
    "    stop.append('br')\n",
    "    #kanoume preprocessing ta dedomena mas\n",
    "    word_tokens=word_tokenize(text.lower()) #kanoume tokenize\n",
    "    #print(word_tokens)\n",
    "    #print(\"Έχουμε \" + str(len(word_tokens))+ \" tokens\")\n",
    "    #filtered_word_tokens = [word for word in word_tokens if word not in stop] #svinoume ta stopwords\n",
    "    #print(\"Αφού αφαιρέσαμε τα stopwords, έχουμε τελικά \" + str(len(filtered_word_tokens)) + \" tokens\")\n",
    "    #print(filtered_word_tokens)\n",
    "    filtered_word_tokens = [re.sub(r'[^A-Za-z]+', '', x) for x in word_tokens]  #svinoume ta punctuations\n",
    "    filtered_word_tokens = [x for x in filtered_word_tokens if ((x.startswith(\"'\")==0 or x.startswith('-') or x.startswith('.')))]  #svinoume ta punctuations\n",
    "\n",
    "    #filtered_word_tokens = [x for x in filtered_word_tokens if x not in ['film','movie']]  #svinoume ta punctuations    \n",
    "    filtered_word_tokens = [word for word in filtered_word_tokens if len(word)>1] #svinoume tis mikres lekseis\n",
    "    #print(filtered_word_tokens)\n",
    "    #print(\"Αφού αφαιρέσαμε τα σημεία στίξης και τις μικρές λέξεις, έχουμε τελικά \" + str(len(filtered_word_tokens)) + \" tokens\")\n",
    "    #print(filtered_word_tokens)\n",
    "   \n",
    "    tagged_filtered_tokens=nltk.pos_tag(filtered_word_tokens) #kanoume pos tag tis lekseis gia syntaktiki analysi\n",
    "    #print(tagged_filtered_tokens)\n",
    "            #ftiaxnoume ta pos tags wste na mpoun san input sto sentisynset\n",
    "    #epeidh ta dedomena tou postag ginontai tuples, ta metatrepoume se lista\n",
    "    newtags=[] #lista me ta nea tags kai idio counter me ta tagged words\n",
    "    for tag in tagged_filtered_tokens:\n",
    "        if tag[1] in set(['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']):\n",
    "            newtags.append('v')\n",
    "        elif tag[1] in set(['JJ', 'JJR', 'JJS']):\n",
    "             newtags.append('a')\n",
    "        elif tag[1] in set(['RB', 'RBR', 'RBS']):\n",
    "             newtags.append('r')\n",
    "        elif tag[1] in set(['NNS', 'NN', 'NNP', 'NNPS']):\n",
    "             newtags.append('n')\n",
    "        else:\n",
    "             newtags.append('a')\n",
    "    \n",
    "    lem_words=[] #edw tha mpoun oi lematized lekseis pou exoume kratisei apo to preprocessing\n",
    "    counter=0 #vazoume ton counter gia na kanoume iterate ta stoixeia tis listas twn tags\n",
    "    for word in tagged_filtered_tokens:    \n",
    "        lem_words.append(wnl.lemmatize(word[0],newtags[counter]))\n",
    "        counter+=1\n",
    "    lem_words = [word for word in lem_words if word not in stop] #svinoume ta stopwords\n",
    "    #print(lem_words)\n",
    "    \n",
    "    return lem_words\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def SentimentAnalysis_Sentiwordnet(text_samples):\n",
    "    my_sentiments=[] #h lista pou tha periexei tosynaisthitiko score kathe keimenou\n",
    "    my_sentiments_class=[]\n",
    "    #print(stopwords)\n",
    "    stop = set(stopwords.words('english'))\n",
    "    for text in text_samples:\n",
    "        #kanoume preprocessing ta dedomena mas\n",
    "        word_tokens=word_tokenize(text.lower()) #kanoume tokenize\n",
    "        #print(\"Έχουμε \" + str(len(word_tokens))+ \" tokens\")\n",
    "        filtered_word_tokens = [word for word in word_tokens if word not in stop] #svinoume ta stopwords\n",
    "        #print(\"Αφού αφαιρέσαμε τα stopwords, έχουμε τελικά \" + str(len(filtered_word_tokens)) + \" tokens\")\n",
    "        #print(filtered_word_tokens)\n",
    "        filtered_word_tokens = [re.sub(r'[^A-Za-z0-9]+', '', x) for x in filtered_word_tokens]  #svinoume ta punctuations\n",
    "        filtered_word_tokens = [word for word in filtered_word_tokens if len(word)>1] #svinoume tis mikres lekseis\n",
    "        #print(\"Αφού αφαιρέσαμε τα σημεία στίξης και τις μικρές λέξεις, έχουμε τελικά \" + str(len(filtered_word_tokens)) + \" tokens\")\n",
    "        #print(filtered_word_tokens)\n",
    "        tagged_filtered_tokens=nltk.pos_tag(filtered_word_tokens) #kanoume pos tag tis lekseis gia syntaktiki analysi\n",
    "        #print(tagged_filtered_tokens)\n",
    "                #ftiaxnoume ta pos tags wste na mpoun san input sto sentisynset\n",
    "        #epeidh ta dedomena tou postag ginontai tuples, ta metatrepoume se lista\n",
    "        newtags=[] #lista me ta nea tags kai idio counter me ta tagged words\n",
    "        for tag in tagged_filtered_tokens:\n",
    "            if tag[1] in set(['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']):\n",
    "                newtags.append('v')\n",
    "            elif tag[1] in set(['JJ', 'JJR', 'JJS']):\n",
    "                 newtags.append('a')\n",
    "            elif tag[1] in set(['RB', 'RBR', 'RBS']):\n",
    "                 newtags.append('r')\n",
    "            elif tag[1] in set(['NNS', 'NN', 'NNP', 'NNPS']):\n",
    "                 newtags.append('n')\n",
    "            else:\n",
    "                 newtags.append('a')\n",
    "                    \n",
    "                    \n",
    "\n",
    "        lem_words=[] #edw tha mpoun oi lematized lekseis pou exoume kratisei apo to preprocessing\n",
    "        counter=0 #vazoume ton counter gia na kanoume iterate ta stoixeia tis listas twn tags\n",
    "        for word in tagged_filtered_tokens:    \n",
    "            lem_words.append(wnl.lemmatize(word[0],newtags[counter]))\n",
    "            counter+=1\n",
    "           # print (newtags)\n",
    "       # new_words_tags_dict = {'word':'synscore'}\n",
    "        #print(newtags,lem_words)\n",
    "            #print(lem_words)\n",
    "        #ypologizoume to synaisthima kathe leksis , mazi me to POSTAG tis\n",
    "        posscore=0\n",
    "        negscore=0\n",
    "        for i in range(len(lem_words)): \n",
    "            synsets = swn.senti_synsets(lem_words[i],newtags[i])\n",
    "            for synst in synsets: #athroizoume ta thetika kai ta arnhtika score kathe leksis\n",
    "                posscore=posscore+synst.pos_score()\n",
    "                negscore=negscore+synst.neg_score()     \n",
    "        my_sentiments.append(posscore-negscore)\n",
    "        #print(my_sentiments)\n",
    "        if (posscore-negscore)>=0:\n",
    "            my_sentiments_class.append(1)\n",
    "        else:\n",
    "            my_sentiments_class.append(0)\n",
    "    print(len(my_sentiments))\n",
    "    return(my_sentiments_class)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "def SentimentAnalysis_Vader(text_samples):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vader_sentiments=[]\n",
    "    vader_class_sentiments=[]\n",
    "    for text in text_samples:\n",
    "        sum=0\n",
    "        #Kovoume kathe keimeno se protaseis wste na vgalei sentiment polarity o vader, ta opoia athroizoume\n",
    "        sentences=text.split('\\n')   \n",
    "        for sentence in sentences:\n",
    "            sent = analyzer.polarity_scores(sentence)\n",
    "            #print(\"{:-<65} {}\".format(sentence, vs))\n",
    "            sum=sum+sent['compound']\n",
    "        average=sum/len(sentences)\n",
    "        vader_sentiments.append(average)\n",
    "        if (average>=0):\n",
    "            vader_class_sentiments.append(1)       \n",
    "        else:\n",
    "            vader_class_sentiments.append(0)\n",
    "        #print(sentiments)\n",
    "        #print(average)\n",
    "    #Data Examination for Unnotated Dataset\n",
    "    print(\"Έχουμε \" + str(vader_class_sentiments.count(1)) + \" χαρούμενα τραγούδια\")\n",
    "    print(\"Έχουμε \" + str(vader_class_sentiments.count(0)) + \" στενάχωρα τραγούδια\")\n",
    "    return vader_class_sentiments\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def classifier_finder(X_train,X_test,y_train,y_test):\n",
    "    classifiers=[]\n",
    "\n",
    "    scores=[]\n",
    "    model1=LogisticRegression(max_iter=1000)\n",
    "    classifiers.append(model1)\n",
    "    model2=MultinomialNB()\n",
    "    classifiers.append(model2)\n",
    "    model4 = tree.DecisionTreeClassifier()\n",
    "    classifiers.append(model4)\n",
    "    model5 = RandomForestClassifier()\n",
    "    classifiers.append(model5)\n",
    "    model6=LinearSVC(max_iter=2000)\n",
    "    classifiers.append(model6)\n",
    "    \n",
    "    \n",
    "    for clf in classifiers:\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        cvec_score = cross_val_score(clf, X_train, y_train, cv=10 )\n",
    "        \n",
    "        \n",
    "        print(\"Η επιτυχία του  %s είναι:  %s\"%(clf, cvec_score.mean()))\n",
    "        scores.append(cvec_score.mean())\n",
    "# DataFrame Accuracy \n",
    "    scores_df = pd.DataFrame()\n",
    "    scores_df['params']= ['Logistisc Regression','Multinomial Naive Bayes','Decision Tree','Random Forest','Linear SVC']\n",
    "    scores_df['scores']= scores\n",
    "    print(scores_df)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def classifier_finder_music(X_train,y_train):\n",
    "    classifiers=[]\n",
    "    #COUNT VECTORIZER\n",
    "\n",
    "    model1=LogisticRegression()\n",
    "    classifiers.append(model1)\n",
    "    model2 = tree.DecisionTreeClassifier()\n",
    "    classifiers.append(model2)\n",
    "    model3 = RandomForestClassifier()\n",
    "    classifiers.append(model3)\n",
    "    model4=LinearSVC()\n",
    "    classifiers.append(model4)\n",
    "    model5 = KNeighborsClassifier()\n",
    "    classifiers.append(model5)\n",
    "\n",
    "\n",
    "    scorelist=[]\n",
    "\n",
    "    for clf in classifiers:\n",
    "        clf.fit(X_train, y_train)\n",
    "        cvec_score = cross_val_score(clf, X_train, y_train, cv=10)\n",
    "        print(\"Τα αποτελέσματα του Cross-Validation για τον {} είναι:  {} \".format(clf,cvec_score.mean()))                                      \n",
    "        scorelist.append(cvec_score.mean())\n",
    "# DataFrame Accuracy \n",
    "    scores_df = pd.DataFrame()\n",
    "    scores_df['params']= ['Logistisc Regression','Decision Tree','Random Forest','Linear SVC','K-nn',]\n",
    "    scores_df['scores']= scorelist\n",
    "    print(scores_df)\n",
    "    \n",
    "\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "#δημιουργούμε συναρτήσεις για να αποθηκεύουμε και να φορτώνουμε  τις λίστες που θα φτιάξουμε με την ανάλυση του vader \n",
    "#και του sentiwordnet\n",
    "def saveList(myList,filename):\n",
    "    np.save(filename,myList)\n",
    "    print(\"Saved successfully!\")\n",
    "    \n",
    "def loadList(filename):\n",
    "    tempNumpyArray=np.load(filename,allow_pickle=True)\n",
    "    return tempNumpyArray.tolist()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#φορτώνουμε τα σετ δεδομένων\n",
    "path1='design_matrix.csv'\n",
    "path2='mean_emotion_ratings.csv'\n",
    "data_input= pd.read_csv(path1)\n",
    "data_class= pd.read_csv(path2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nro</th>\n",
       "      <th>Register</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Tempo</th>\n",
       "      <th>Soundlevel</th>\n",
       "      <th>Articulation</th>\n",
       "      <th>Timbre</th>\n",
       "      <th>Melody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Nro  Register  Mode  Tempo  Soundlevel  Articulation  Timbre  Melody\n",
       "0    1         4     1      4           4             2       2       4\n",
       "1    2         5     1      4           1             1       2       2\n",
       "2    3         2     2      5           1             1       2       1\n",
       "3    4         1     1      5           4             4       1       2\n",
       "4    5         3     2      1           3             2       2       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_input.head(5) #vlepoume oti prepei na allaksoume to mode, kathws periexei kathgoriaka dedomena, kai na diagrapsoume to Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nro</th>\n",
       "      <th>Scary</th>\n",
       "      <th>Happy</th>\n",
       "      <th>Sad</th>\n",
       "      <th>Peaceful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.2889</td>\n",
       "      <td>4.4667</td>\n",
       "      <td>1.7111</td>\n",
       "      <td>3.1333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0667</td>\n",
       "      <td>5.4444</td>\n",
       "      <td>1.4889</td>\n",
       "      <td>4.4889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0222</td>\n",
       "      <td>1.4889</td>\n",
       "      <td>3.7778</td>\n",
       "      <td>2.7111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2.2889</td>\n",
       "      <td>4.1111</td>\n",
       "      <td>1.2667</td>\n",
       "      <td>1.4889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.4000</td>\n",
       "      <td>1.4667</td>\n",
       "      <td>5.0444</td>\n",
       "      <td>3.8444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Nro   Scary   Happy     Sad  Peaceful\n",
       "0    1  1.2889  4.4667  1.7111    3.1333\n",
       "1    2  1.0667  5.4444  1.4889    4.4889\n",
       "2    3  2.0222  1.4889  3.7778    2.7111\n",
       "3    4  2.2889  4.1111  1.2667    1.4889\n",
       "4    5  1.4000  1.4667  5.0444    3.8444"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_class.head(5) # vlepoume oti prepei na diagrapsoume to number kai na doume pws tha xeiristoume ta multiclass data afou theloume diadikh synaisthimatikh analysh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#θα μετατρέψουμε το Mode από τα δεδομένα μας, σε δυο διαφορετικά διανύσματα εισόδου, το Major και το Minor.Αν η κλιμακα ειναι\n",
    "#ματζορε τοτε το Major θα παιρνει τιμή 1, και το Minor τιμη 0. αντιθετα, αν η συγχορδία ειναι μινορε, το Major θα παιρνει την \n",
    "#τιμη 0 και το Minor την τιμή 1\n",
    "#ταυτοχρονα διαγραφουμε τις στηλες Nro από τα δεδομένα εισόδου και εξόδου μας, καθώς και το Mode από τα δεδομένα εισόδου μας\n",
    "Major=[]\n",
    "Minor=[]\n",
    "#αρχικα δημιουργουμε τις αντιστοιχες λιστες\n",
    "for i in range (0,len(data_input)):\n",
    "    if (data_input['Mode'][i]==2):\n",
    "        Major.append(1)\n",
    "        Minor.append(0)\n",
    "    else:\n",
    "        Major.append(0)\n",
    "        Minor.append(1)\n",
    "#και μετα τις περναμε στο dataframe μας\n",
    "data_input['Major']=Major\n",
    "data_input['Minor']=Minor\n",
    "if {'Nro'}.issubset(data_input.columns):\n",
    "    data_input=data_input.drop(['Nro'], axis=1)\n",
    "if {'Mode'}.issubset(data_input.columns):\n",
    "    data_input=data_input.drop(['Mode'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Register</th>\n",
       "      <th>Tempo</th>\n",
       "      <th>Soundlevel</th>\n",
       "      <th>Articulation</th>\n",
       "      <th>Timbre</th>\n",
       "      <th>Melody</th>\n",
       "      <th>Major</th>\n",
       "      <th>Minor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Register  Tempo  Soundlevel  Articulation  Timbre  Melody  Major  Minor\n",
       "0         4      4           4             2       2       4      0      1\n",
       "1         5      4           1             1       2       2      0      1\n",
       "2         2      5           1             1       2       1      1      0\n",
       "3         1      5           4             4       1       2      0      1\n",
       "4         3      1           3             2       2       1      1      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_input.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Έχουμε 22 περιπτώσεις όπου happy>sad και peaceful < scary\n",
      "Συνεπώς υπάρχει αμφισημία μεταξύ των κλάσεων από το dataset μας.\n",
      "Θα δημιουργήσουμε 3 διαφορετικές περιπτώσεις με βάση τις υπάρχουσες κλάσεις, ώστε να καλύψουμε τις διαφορετικές εκδοχές\n"
     ]
    }
   ],
   "source": [
    "#classes examination\n",
    "\n",
    "count=0\n",
    "for i in range (0,len(data_class)):\n",
    "    if ((data_class['Happy'][i]>data_class['Sad'][i]) and (data_class['Peaceful'][i]<data_class['Scary'][i])):\n",
    "        count=count+1\n",
    "print(\"Έχουμε \" + str(count) + \" περιπτώσεις όπου happy>sad και peaceful < scary\")#Παρατηρούμε ότι στο dataset εχουμε περιπτωσεις οπου η κλαση happy kai peaceful den symvadizoun san dyadika antitheta panta.\n",
    "print(\"Συνεπώς υπάρχει αμφισημία μεταξύ των κλάσεων από το dataset μας.\")\n",
    "#synepws tha ftiaksoume 3 diaforetika classiciation gia ta dedomena pou exoume\n",
    "print(\"Θα δημιουργήσουμε 3 διαφορετικές περιπτώσεις με βάση τις υπάρχουσες κλάσεις, ώστε να καλύψουμε τις διαφορετικές εκδοχές\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Η απλή κλάση συναισθήματος που φτιάξαμε περιλαμβάνει: \n",
      "Έχουμε 86 χαρούμενα τραγούδια\n",
      "Έχουμε 114 στενάχωρα τραγούδια\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#θα δημιουργήσουμε διαφορετικές κλάσεις ώστε να καλύψουμε όλο το φάσμα των περιπτώσεων, δημιουργώντας 3 περιπτώσεις:\n",
    "#Μία πιο απλή, (happy vs sad)\n",
    "#μια πιο εξειδικευμένη (happy+peaceful vs peaceful+scary)\n",
    "#και η πιο εξειδικευμένη θα ακολουθεί την εξής λογική\n",
    "#αν happy>sad και peaceful >scary ->happy\n",
    "#aν sad>happy και scary>peaceful -> sad\n",
    "#αν happy>sad scary>peaceful τότε αν |happy - sad | *150%> |scary - peaceful| *50%-> happy, else sad\n",
    "\n",
    "#τροποποιούμε το dataset των κλάσεων έτσι ώστε να έχουμε 2 κλάσεις, 1 για χαρούμενο \n",
    "#και 0 για στενάχωρο τραγούδι, αντίθετα με τις 4 κλάσεις που έχουμε ήδη φτιάξει\n",
    "music_class_simple=[]\n",
    "\n",
    "for i in range (0,len(data_class)):\n",
    "    if (data_class['Happy'][i]>data_class['Sad'])[i]:\n",
    "        music_class_simple.append(1)\n",
    "    else:\n",
    "        music_class_simple.append(0)\n",
    "print(\"Η απλή κλάση συναισθήματος που φτιάξαμε περιλαμβάνει: \")\n",
    "#print(music_class_simple)\n",
    "#τώρα στο music_class_simple εχουμε τις κλάσεις των συναισθημάτων μας\n",
    "\n",
    "print(\"Έχουμε \" + str(music_class_simple.count(1)) + \" χαρούμενα τραγούδια\")\n",
    "print(\"Έχουμε \" + str(music_class_simple.count(0)) + \" στενάχωρα τραγούδια\")\n",
    "sns.countplot(music_class_simple,label=\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Η δεύτερη κλάση συναισθήματος που φτιάξαμε περιλαμβάνει: \n",
      "Έχουμε 92 χαρούμενα τραγούδια\n",
      "Έχουμε 108 στενάχωρα τραγούδια\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "music_class_better=[]\n",
    "#print(data_class)\n",
    "for i in range (0,len(data_class)):\n",
    "    if ((data_class['Happy'][i] + data_class['Peaceful'][i]) > (data_class['Sad'] + data_class['Scary'][i])[i]):\n",
    "        music_class_better.append(1)\n",
    "    else:\n",
    "        music_class_better.append(0)\n",
    "print(\"Η δεύτερη κλάση συναισθήματος που φτιάξαμε περιλαμβάνει: \")\n",
    "#print(music_class_better)\n",
    "\n",
    "print(\"Έχουμε \" + str(music_class_better.count(1)) + \" χαρούμενα τραγούδια\")\n",
    "print(\"Έχουμε \" + str(music_class_better.count(0)) + \" στενάχωρα τραγούδια\")\n",
    "sns.countplot(music_class_better,label=\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Η κλάση συναισθήματος που φτιάξαμε περιλαμβάνει: \n",
      "Έχουμε 102 χαρούμενα τραγούδια\n",
      "Έχουμε 98 στενάχωρα τραγούδια\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD3CAYAAAAQYlNPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADJJJREFUeJzt3V+MXIdVgPFvHXs3iDgGqaQKUmigKAepK6ViCk7TOl6qpMWl4KhILQ+oaqvSFwuVqlJKKgfaqDxUKq5IaQVKMA5IPLSuwp9YprWgcpxAaTpKhUe4xxARRaIRShPcOkDHsT08zJhM3N14vOu5d3bP95NWmrkzc+c8jPbb+2fuzg0GAyRJdW1qewBJUrsMgSQVZwgkqThDIEnFbW57gMvV7XYXgJ8DngHOtTyOJK0XVwHXA493Op3++APrLgQMI3Cs7SEkaZ3aATw6vmA9huAZgJtuuon5+fm2Z5GkdeHMmTOcPHkSRr9Dx63HEJwDmJ+fZ2Fhoe1ZJGm9+YFd6h4slqTiDIEkFWcIJKk4QyBJxRkCSSrOEEhScYZAkoozBJJU3Hr8Qpm0oR04cm/bI2gGvfeO35nauqcagojYDnwqM5ci4qeBA8AA6AF7MvN8RPwu8EvAWeC3MvPr05xJkvRyU9s1FBF3AQ8AV48W7QP2ZuYOYA7YHRE/C+wEtgO/BnxuWvNIkpY3zWMETwLvHLvfAY6Obh8GbgfeDHwlMweZ+TSwOSJ+bIozSZIuMrVdQ5n5pYi4cWzRXGYORrdPA9uAa4Hnxp5zYfmzl1p/r9e7QpNK0uzrdrtTW3eTB4vPj93eCpwCvje6ffHyS1pcXPTqo9qQjh851PYImkGdTmdNr+/3+yv+Ad3k6aNPRMTS6PYuhv9c5jHgbRGxKSJ+AtiUmd9pcCZJKq/JLYKPAPdHxDxwAjiYmeci4hjwjwyjtKfBeSRJTDkEmfkUcMvo9kmGZwhd/JyPAx+f5hzLefWezzT9lppx//m5D7c9gtQKv1ksScUZAkkqzhBIUnGGQJKKMwSSVJwhkKTiDIEkFWcIJKk4QyBJxRkCSSrOEEhScYZAkoozBJJUnCGQpOIMgSQVZwgkqThDIEnFGQJJKs4QSFJxhkCSijMEklScIZCk4gyBJBVnCCSpOEMgScUZAkkqzhBIUnGGQJKKMwSSVJwhkKTiDIEkFWcIJKk4QyBJxW1u8s0iYgvwIHAjcA74DeAscAAYAD1gT2aeb3IuSaqs6S2CtwObM/NW4F7g94B9wN7M3AHMAbsbnkmSSmt0iwA4CWyOiE3AtcCLwC3A0dHjh4G3Ag9dakW9Xm9aM6qobrfb9gjSiqb5+Ww6BC8w3C30LeBVwDuA2zJzMHr8NLBtkhUtLi6ysLCw+kn2P7L612pD6nQ6bY8AwPEjh9oeQTNorZ/Pfr+/4h/QTe8a+jDw5cy8CbiZ4fGC+bHHtwKnGp5JkkprOgT/BXx3dPt5YAvwREQsjZbtAo41PJMkldb0rqHPAPsj4hjDLYGPAd8A7o+IeeAEcLDhmSSptEZDkJkvAO9a5qGdTc4hSXqJXyiTpOIMgSQVZwgkqThDIEnFGQJJKs4QSFJxhkCSijMEklScIZCk4gyBJBVnCCSpOEMgScUZAkkqzhBIUnGGQJKKMwSSVJwhkKTiDIEkFWcIJKk4QyBJxRkCSSrOEEhScYZAkoozBJJUnCGQpOIMgSQVZwgkqThDIEnFGQJJKs4QSFJxhkCSijMEklTc5qbfMCLuBn4FmAc+DxwFDgADoAfsyczzTc8lSVU1ukUQEUvArcCbgJ3ADcA+YG9m7gDmgN1NziRJ1TW9a+htwHHgIeBvgIeBDsOtAoDDwO0NzyRJpTW9a+hVwGuAdwA/Cfw1sCkzB6PHTwPbJllRr9ebyoCqq9vttj2CtKJpfj6bDsFzwLcy8wyQEfF9hruHLtgKnJpkRYuLiywsLKx+kv2PrP612pA6nU7bIwBw/MihtkfQDFrr57Pf76/4B/REu4Yi4rPLLHtwFbM8CvxiRMxFxI8DPwz83ejYAcAu4Ngq1itJWqVX3CKIiAeAnwLeEBGvG3toCxPuwhmXmQ9HxG3A1xlGaA/w78D9ETEPnAAOXu56JUmrd6ldQ58EbgT+APjE2PKzDH9pX7bMvGuZxTtXsy5J0tq9Yggy8yngKeDmiLiW4VbA3Ojha4DnpzmcJGn6JjpYPPoS2N0MD/ZeMGC420iStI5NetbQB4DXZuaz0xxGktS8Sb9Q9jTuBpKkDWnSLYJ/BR6NiK8C37+wMDPvncpUkqTGTBqC/xj9wEsHiyVJG8BEIcjMT1z6WZKk9WjSs4bOMzxLaNy3M/OG5Z4vSVo/Jt0i+P+DyhGxBbgTeOO0hpIkNeeyL0OdmS9m5heBt0xhHklSwybdNfSesbtzwOuAF6cykSSpUZOeNfQLY7cHwHeAd1/5cSRJTZv0GMH7RscGYvSaXmaenepkkqRGTPr/CDoMv1T2IPCnwNMRsX2ag0mSmjHprqH7gHdn5j8BRMQtwGeBn5/WYJKkZkx61tA1FyIAkJlfA66ezkiSpCZNGoLnI2L3hTsRcScvvyS1JGmdmnTX0AeBhyPiTxiePjoAbp3aVJKkxky6RbAL+B/gNQxPJX0WWJrSTJKkBk0agg8Cb8rM/87MfwY6wG9ObyxJUlMmDcEW4MzY/TP84EXoJEnr0KTHCP4S+PuI+ALDAPwq8FdTm0qS1JiJtggy86MMv0sQwGuB+zLznmkOJklqxqRbBGTmQeDgFGeRJLXgsi9DLUnaWAyBJBVnCCSpOEMgScUZAkkqzhBIUnGGQJKKMwSSVJwhkKTiJv5m8ZUUEdcBXeAO4CxwgOE1jHrAnsw838ZcklRR41sEEbEF+GPgf0eL9gF7M3MHw396s3ul10qSrrw2dg19Gvgj4Nuj+x3g6Oj2YeD2FmaSpLIa3TUUEe8Fns3ML0fE3aPFc5l54X8bnAa2TbKuXq83hQlVWbfbbXsEaUXT/Hw2fYzg/cAgIm4HXg/8GXDd2ONbgVOTrGhxcZGFhYXVT7L/kdW/VhtSp9NpewQAjh851PYImkFr/Xz2+/0V/4BudNdQZt6WmTszcwn4JvAe4HBELI2esgs41uRMklRdK2cNXeQjwP0RMQ+cwP95IEmNai0Eo62CC3a2NYckVecXyiSpOEMgScUZAkkqzhBIUnGGQJKKMwSSVJwhkKTiDIEkFWcIJKk4QyBJxRkCSSrOEEhScYZAkoozBJJUnCGQpOIMgSQVZwgkqThDIEnFGQJJKs4QSFJxhkCSijMEklScIZCk4gyBJBVnCCSpOEMgScUZAkkqzhBIUnGGQJKKMwSSVJwhkKTiDIEkFWcIJKm4zU2+WURsAfYDNwILwCeBfwEOAAOgB+zJzPNNziVJlTW9RfDrwHOZuQPYBfwhsA/YO1o2B+xueCZJKq3RLQLgi8DBsftngQ5wdHT/MPBW4KFLrajX613x4VRbt9ttewRpRdP8fDYagsx8ASAitjIMwl7g05k5GD3lNLBtknUtLi6ysLCw+mH2P7L612pD6nQ6bY8AwPEjh9oeQTNorZ/Pfr+/4h/QjR8sjogbgK8Cf56ZfwGMHw/YCpxqeiZJqqzREETEq4GvAB/NzP2jxU9ExNLo9i7gWJMzSVJ1TR8j+Bjwo8A9EXHPaNmHgPsiYh44wcuPIUiSpqzpYwQfYviL/2I7m5xDkvQSv1AmScUZAkkqzhBIUnGGQJKKMwSSVJwhkKTiDIEkFWcIJKk4QyBJxRkCSSrOEEhScYZAkoozBJJUnCGQpOIMgSQVZwgkqThDIEnFGQJJKs4QSFJxhkCSijMEklScIZCk4gyBJBVnCCSpOEMgScUZAkkqzhBIUnGGQJKKMwSSVJwhkKTiDIEkFWcIJKm4zW0PABARm4DPAzcDfeADmflv7U4lSTXMyhbBncDVmflG4LeB3295HkkqYya2CIA3A38LkJlfi4g3vMJzrwI4c+bMmt7wumsW1vR6bTz9fr/tEQDYsumH2h5BM2itn8+x35lXXfzYrITgWuC7Y/fPRcTmzDy7zHOvBzh58uSa3vDAu7av6fXaeHq9XtsjAPAzP/KWtkfQDLqCn8/rgSfHF8xKCL4HbB27v2mFCAA8DuwAngHOTXswSdogrmIYgccvfmBWQvAY8MvAFyLiFuD4Sk/sdDp94NGmBpOkDeTJ5RbOSggeAu6IiH8A5oD3tTyPJJUxNxgM2p5BktSiWTl9VJLUEkMgScUZAkkqblYOFqthXtZDsy4itgOfysyltmfZ6NwiqMvLemhmRcRdwAPA1W3PUoEhqOtll/UAXumyHlLTngTe2fYQVRiCupa9rEdbw0jjMvNLwIttz1GFIajrci7rIWkDMwR1PQa8HeBSl/WQtLG5K6AuL+shCfASE5JUnruGJKk4QyBJxRkCSSrOEEhScYZAkoozBJJUnCGQpOL+D40YdBhA/m+QAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "music_class_best=[]\n",
    "for i in range (0,len(data_class)):\n",
    "    if (((data_class['Happy'][i]) >= (data_class['Sad'][i])) and ((data_class['Peaceful'][i]) >= (data_class['Scary'][i]))):\n",
    "            music_class_best.append(1)\n",
    "    elif (((data_class['Happy'][i]) < (data_class['Sad'][i])) and ((data_class['Peaceful'][i]) < (data_class['Scary'][i]))):\n",
    "            music_class_best.append(0)\n",
    "    elif (((data_class['Happy'][i]) >= (data_class['Sad'][i])) and ((data_class['Peaceful'][i]) < (data_class['Scary'][i]))):\n",
    "        if (1,5*((data_class['Happy'][i]) - (data_class['Sad'][i])) >= (0.5*(data_class['Scary'][i]) - (data_class['Peaceful'][i]))):\n",
    "            music_class_best.append(1)\n",
    "        elif (1,5*(data_class['Happy'][i]) - (data_class['Sad'][i]) < 0.5*((data_class['Scary'][i]) - (data_class['Peaceful'][i]))):\n",
    "            music_class_best.append(0)\n",
    "    elif (((data_class['Happy'][i]) < (data_class['Sad'][i])) and ((data_class['Peaceful'][i]) >= (data_class['Scary'][i]))):\n",
    "        if ((data_class['Sad'][i]) - (data_class['Happy'][i]) >= ((data_class['Peaceful'][i]) - (data_class['Scary'][i]))):\n",
    "            music_class_best.append(0)\n",
    "        elif ((data_class['Sad'][i]) - (data_class['Happy'][i]) < ((data_class['Peaceful'][i]) - (data_class['Scary'][i]))):\n",
    "            music_class_best.append(1)\n",
    "\n",
    "\n",
    "print(\"Η κλάση συναισθήματος που φτιάξαμε περιλαμβάνει: \")\n",
    "#print(music_class_best)\n",
    "print(\"Έχουμε \" + str(music_class_best.count(1)) + \" χαρούμενα τραγούδια\")\n",
    "print(\"Έχουμε \" + str(music_class_best.count(0)) + \" στενάχωρα τραγούδια\")\n",
    "sns.countplot(music_class_best,label=\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Τα αποτελέσματα του Cross-Validation για τον LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False) είναι:  0.8935539215686275 \n",
      "Τα αποτελέσματα του Cross-Validation για τον DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best') είναι:  0.8255882352941176 \n",
      "Τα αποτελέσματα του Cross-Validation για τον RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False) είναι:  0.868970588235294 \n",
      "Τα αποτελέσματα του Cross-Validation για τον LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0) είναι:  0.8998039215686274 \n",
      "Τα αποτελέσματα του Cross-Validation για τον KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform') είναι:  0.8873039215686275 \n",
      "                 params    scores\n",
      "0  Logistisc Regression  0.893554\n",
      "1         Decision Tree  0.825588\n",
      "2         Random Forest  0.868971\n",
      "3            Linear SVC  0.899804\n",
      "4                  K-nn  0.887304\n"
     ]
    }
   ],
   "source": [
    "#χωρίζουμε την πρώτη κλάση σε train/test σετ. τα ονομάζουμε X_train_s από το simple καθώς είναι προιόν της πιο απλής ανάλυσης\n",
    "\n",
    "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(data_input, music_class_simple, test_size=0.2,random_state=5)\n",
    "#κανονικοποιούμε τα δεδομένα εκπαίδευσης και ελέγχου, για να πετύχουμε καλύτερα αποτελέσματα\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "X_train_scaled1 = scaler1.fit_transform(X_train_s)\n",
    "X_test_scaled1 = scaler1.transform(X_test_s)\n",
    "#print(X_scaled)\n",
    "\n",
    "classifier_finder_music(X_train_scaled1,y_train_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Τα αποτελέσματα του Cross-Validation για τον LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False) είναι:  0.8396078431372549 \n",
      "Τα αποτελέσματα του Cross-Validation για τον DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best') είναι:  0.7899264705882354 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Τα αποτελέσματα του Cross-Validation για τον RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False) είναι:  0.8204411764705883 \n",
      "Τα αποτελέσματα του Cross-Validation για τον LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0) είναι:  0.8396078431372549 \n",
      "Τα αποτελέσματα του Cross-Validation για τον KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform') είναι:  0.8074754901960786 \n",
      "                 params    scores\n",
      "0  Logistisc Regression  0.839608\n",
      "1         Decision Tree  0.789926\n",
      "2         Random Forest  0.820441\n",
      "3            Linear SVC  0.839608\n",
      "4                  K-nn  0.807475\n"
     ]
    }
   ],
   "source": [
    "#δημιουργούμε τα διανύσματα εκπαίδευσης και ελέγχου από τη δεύτερη ανάλυση. τα ονομάζουμε με κατάληξη _b από το better\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(data_input, music_class_better, test_size=0.2,random_state=6)\n",
    "\n",
    "#κανονικοποιούμε τα διανύσματα μας\n",
    "scaler2 = StandardScaler()\n",
    "X_train_scaled2 = scaler2.fit_transform(X_train_b)\n",
    "X_test_scaled2 = scaler2.transform(X_test_b)\n",
    "#print(X_scaled)\n",
    "\n",
    "classifier_finder_music(X_train_scaled2,y_train_b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  import sys\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Τα αποτελέσματα του Cross-Validation για τον LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False) είναι:  0.8325490196078432 \n",
      "Τα αποτελέσματα του Cross-Validation για τον DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best') είναι:  0.8125490196078433 \n",
      "Τα αποτελέσματα του Cross-Validation για τον RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False) είναι:  0.8768627450980393 \n",
      "Τα αποτελέσματα του Cross-Validation για τον LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0) είναι:  0.8325490196078432 \n",
      "Τα αποτελέσματα του Cross-Validation για τον KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform') είναι:  0.8384313725490197 \n",
      "                 params    scores\n",
      "0  Logistisc Regression  0.832549\n",
      "1         Decision Tree  0.812549\n",
      "2         Random Forest  0.876863\n",
      "3            Linear SVC  0.832549\n",
      "4                  K-nn  0.838431\n"
     ]
    }
   ],
   "source": [
    "#χωρίζουμε το 3ο υποσετ σε διανύσματα εκπαίδευσης και ελέγχου.η κατάληξη _B Προκύπτει από το Best καθώς είναι η πιο συγκεκριμένη μας ανάλυση για τις συναισθηματικές κλάσεις\n",
    "\n",
    "X_train_B, X_test_B, y_train_B, y_test_B = train_test_split(data_input, music_class_best, test_size=0.2,random_state=7)\n",
    "\n",
    "scaler3 = StandardScaler()\n",
    "X_train_scaled3 = scaler3.fit_transform(X_train_B)\n",
    "X_test_scaled3 = scaler3.transform(X_test_B)\n",
    "#print(X_scaled)\n",
    "\n",
    "classifier_finder_music(X_train_scaled3,y_train_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#θελουμε να κανουμε optimize : \n",
    "\n",
    "#L-SVC, L-SVC,RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.900000 using {'C': 0.5, 'dual': True, 'max_iter': 110}\n",
      "Execution time: 5.372822999954224 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.5, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=110,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Θα κάνουμε GridSearch για τον LinearSVC για την πρώτη περίπτωση\n",
    "#οριζουμε τις μεγιστες επαναληψεις\n",
    "max_iter=[110,200,500,1000,1500,2000]\n",
    "dual=[True,False]\n",
    "#οριζουμε το πεναλτυ\n",
    "C = [0.1,0.5,1.0,1.5,2.0,2.5,5,10]\n",
    "param_grid = dict(dual=dual,max_iter=max_iter,C=C)\n",
    "\n",
    "lSVC=LinearSVC()\n",
    "grid = GridSearchCV(estimator=lSVC, param_grid=param_grid, cv = 10, n_jobs=-1)\n",
    "\n",
    "start_time = time.time()\n",
    "grid_result = grid.fit(X_train_scaled1, y_train_s)\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "print(\"Execution time: \" + str((time.time() - start_time)) + ' ms')\n",
    "best_clf1 = LinearSVC(**grid_result.best_params_)\n",
    "best_clf1.fit(X_train_scaled1,y_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.843750 using {'C': 5, 'dual': True, 'max_iter': 200}\n",
      "Execution time: 1.9743807315826416 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=5, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=200,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Θα κάνουμε GridSearch για τον LinearSVC για την δεύτερη περίπτωση\n",
    "#οριζουμε τις μεγιστες επαναληψειςmax_iter=[110,200,500,1000,1500,2000]\n",
    "dual=[True,False]\n",
    "C = [0.1,0.5,1.0,1.5,2.0,2.5,5,10]\n",
    "param_grid = dict(dual=dual,max_iter=max_iter,C=C)\n",
    "\n",
    "lSVC=LinearSVC()\n",
    "grid = GridSearchCV(estimator=lSVC, param_grid=param_grid, cv = 10, n_jobs=-1)\n",
    "\n",
    "start_time = time.time()\n",
    "grid_result = grid.fit(X_train_scaled2, y_train_b)\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "print(\"Execution time: \" + str((time.time() - start_time)) + ' ms')\n",
    "best_clf2 = LinearSVC(**grid_result.best_params_)\n",
    "best_clf2.fit(X_train_scaled2,y_train_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:  5.6min finished\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.868750 using {'criterion': 'gini', 'max_depth': 7, 'max_features': 'auto', 'n_estimators': 200}\n",
      "Execution time: 337.2352831363678 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Θα κάνουμε GridSearch για τον RandomForestClassifier για την τρίτη περίπτωση\n",
    "#οριζουμε τις μεγιστες επαναληψεις\n",
    "param_grid = { \n",
    "    'n_estimators': [200, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "rfc = RandomForestClassifier()\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 10,verbose=1)\n",
    "start_time = time.time()\n",
    "CV_rfc.fit(X_train_scaled3, y_train_B)\n",
    "\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (CV_rfc.best_score_, CV_rfc.best_params_))\n",
    "print(\"Execution time: \" + str((time.time() - start_time)) + ' ms')\n",
    "best_clf3 = RandomForestClassifier(**CV_rfc.best_params_)\n",
    "best_clf3.fit(X_train_scaled3, y_train_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Επιτυχία πρώτου ταξινομητη, LinearSVC :\n",
      "Η επιτυχία του ταξινομητή για το πρώτο σετ δεδομένων είναι 0.8\n",
      "Η επιτυχία του ταξινομητή για το δεύτερο σετ δεδομένων είναι 0.85\n",
      "Η επιτυχία του ταξινομητή για το τρίτο σετ δεδομένων είναι 0.825\n",
      "\n",
      "Βρίσκουμε τον μέσο όρο του ταξινομητή για τα διαφορετικά σετ δεδομένων\n",
      "Μέσος Όρος πρώτου ταξινομητή: 0.8249999999999998\n",
      "\n",
      "Επιτυχία δεύτερου ταξινομητη, LinearSVC :\n",
      "Η επιτυχία του ταξινομητή για το πρώτο σετ δεδομένων είναι 0.825\n",
      "Η επιτυχία του ταξινομητή για το δεύτερο σετ δεδομένων είναι 0.825\n",
      "Η επιτυχία του ταξινομητή για το τρίτο σετ δεδομένων είναι 0.875\n",
      "\n",
      "Βρίσκουμε τον μέσο όρο του ταξινομητή για τα διαφορετικά σετ δεδομένων\n",
      "Mέσος Όρος δεύτερου ταξινομητή: 0.8416666666666667\n",
      "\n",
      "Σκορ 3ου ταξινομητη, RandomForest :\n",
      "Η επιτυχία του ταξινομητή στο πρώτο σετ δεδομένων είναι 0.875\n",
      "Η επιτυχία του ταξινομητή στο δεύτερο σετ δεδομένων είναι 0.95\n",
      "Η επιτυχία του ταξινομητή στο τρίτο σετ δεδομένων είναι 0.875\n",
      "\n",
      "Βρίσκουμε τον μέσο όρο του ταξινομητή για τα διαφορετικά σετ δεδομένων\n",
      "Μέσος όρος τρίτου ταξινομητή: 0.9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ελέγχουμε τους classifiers μας. Επειδή έχουμε εκπαιδεύσει και τους 3 στο ίδιο training data με διαφορετικά labels,\n",
    "#θα δημιουργήσουμε 3 ελέγχους ,με 3 σκορ τα οποία θα πολλαπλασιάσουμε μεταξύ τους για να βρούμε το καλύτερο\n",
    "#παίρνουμε σαν σωστή κλάση την 1η\n",
    "# make a prediction\n",
    "ypred1 = best_clf1.predict(X_test_scaled1)\n",
    "ypred2 = best_clf1.predict(X_test_scaled2)\n",
    "ypred3 = best_clf1.predict(X_test_scaled3)\n",
    "#αποθηκευουμε τα σκορ\n",
    "score1 = accuracy_score(y_test_s, ypred1)\n",
    "score2 =accuracy_score(y_test_b, ypred2)\n",
    "score3 = accuracy_score(y_test_B, ypred3)\n",
    "print(\"Επιτυχία πρώτου ταξινομητη, LinearSVC :\")\n",
    "print(\"Η επιτυχία του ταξινομητή για το πρώτο σετ δεδομένων είναι {}\".format(accuracy_score(y_test_s, ypred1)))\n",
    "print(\"Η επιτυχία του ταξινομητή για το δεύτερο σετ δεδομένων είναι {}\".format(accuracy_score(y_test_b, ypred2)))\n",
    "print(\"Η επιτυχία του ταξινομητή για το τρίτο σετ δεδομένων είναι {}\".format(accuracy_score(y_test_B, ypred3)))\n",
    "print()\n",
    "print(\"Βρίσκουμε τον μέσο όρο του ταξινομητή για τα διαφορετικά σετ δεδομένων\")\n",
    "print(\"Μέσος Όρος πρώτου ταξινομητή: {}\".format((score1+score2+score3)/3))\n",
    "print()\n",
    "# Φτιάχνουμε DF με τα αποτελέσματα μας \n",
    "accuracy_df1 = pd.DataFrame()\n",
    "accuracy_df1['Σετ']= ['1ο σετ δεδομένων','2ο σετ δεδομένων','3ο σετ δεδομένων',]\n",
    "accuracy_df1['scores']= [score1,score2,score3]\n",
    "\n",
    "#ελέγχουμε τους classifiers μας. Επειδή έχουμε εκπαιδεύσει και τους 3 στο ίδιο training data με διαφορετικά labels,\n",
    "#θα δημιουργήσουμε 3 ελέγχους ,με 3 σκορ τα οποία θα πολλαπλασιάσουμε μεταξύ τους για να βρούμε το καλύτερο\n",
    "#παίρνουμε σαν σωστή κλάση την 2η\n",
    "# make a prediction\n",
    "ypred1 = best_clf2.predict(X_test_scaled1)\n",
    "ypred2 = best_clf2.predict(X_test_scaled2)\n",
    "ypred3 = best_clf2.predict(X_test_scaled3)\n",
    "#αποθηκευουμε τα σκορ\n",
    "score1 = accuracy_score(y_test_s, ypred1)\n",
    "score2 =accuracy_score(y_test_b, ypred2)\n",
    "score3 = accuracy_score(y_test_B, ypred3)\n",
    "print(\"Επιτυχία δεύτερου ταξινομητη, LinearSVC :\")\n",
    "print(\"Η επιτυχία του ταξινομητή για το πρώτο σετ δεδομένων είναι {}\".format(accuracy_score(y_test_s, ypred1)))\n",
    "print(\"Η επιτυχία του ταξινομητή για το δεύτερο σετ δεδομένων είναι {}\".format(accuracy_score(y_test_b, ypred2)))\n",
    "print(\"Η επιτυχία του ταξινομητή για το τρίτο σετ δεδομένων είναι {}\".format(accuracy_score(y_test_B, ypred3)))\n",
    "print()\n",
    "print(\"Βρίσκουμε τον μέσο όρο του ταξινομητή για τα διαφορετικά σετ δεδομένων\")\n",
    "print(\"Mέσος Όρος δεύτερου ταξινομητή: {}\".format((score1+score2+score3)/3))\n",
    "print()\n",
    "# Φτιάχνουμε DF με τα αποτελέσματα μας \n",
    "accuracy_df2 = pd.DataFrame()\n",
    "accuracy_df2['Σετ']= ['1ο σετ δεδομένων','2ο σετ δεδομένων','3ο σετ δεδομένων',]\n",
    "accuracy_df2['scores']= [score1,score2,score3]\n",
    "\n",
    "#ελέγχουμε τους classifiers μας. Επειδή έχουμε εκπαιδεύσει και τους 3 στο ίδιο training data με διαφορετικά labels,\n",
    "#θα δημιουργήσουμε 3 ελέγχους ,με 3 σκορ τα οποία θα πολλαπλασιάσουμε μεταξύ τους για να βρούμε το καλύτερο\n",
    "#παίρνουμε σαν σωστή κλάση την 3η\n",
    "# make a prediction\n",
    "ypred1 = best_clf3.predict(X_test_scaled1)\n",
    "ypred2 = best_clf3.predict(X_test_scaled2)\n",
    "ypred3 = best_clf3.predict(X_test_scaled3)\n",
    "#αποθηκευουμε τα σκορ\n",
    "score1 = accuracy_score(y_test_s, ypred1)\n",
    "score2 =accuracy_score(y_test_b, ypred2)\n",
    "score3 = accuracy_score(y_test_B, ypred3)\n",
    "print(\"Σκορ 3ου ταξινομητη, RandomForest :\")\n",
    "print(\"Η επιτυχία του ταξινομητή στο πρώτο σετ δεδομένων είναι {}\".format(accuracy_score(y_test_s, ypred1)))\n",
    "print(\"Η επιτυχία του ταξινομητή στο δεύτερο σετ δεδομένων είναι {}\".format(accuracy_score(y_test_b, ypred2)))\n",
    "print(\"Η επιτυχία του ταξινομητή στο τρίτο σετ δεδομένων είναι {}\".format(accuracy_score(y_test_B, ypred3)))\n",
    "print()\n",
    "print(\"Βρίσκουμε τον μέσο όρο του ταξινομητή για τα διαφορετικά σετ δεδομένων\")\n",
    "print(\"Μέσος όρος τρίτου ταξινομητή: {}\".format((score1+score2+score3)/3))\n",
    "print()\n",
    "# Φτιάχνουμε DF με τα αποτελέσματα μας \n",
    "accuracy_df3 = pd.DataFrame()\n",
    "accuracy_df3['Σετ']= ['1ο σετ δεδομένων','2ο σετ δεδομένων','3ο σετ δεδομένων',]\n",
    "accuracy_df3['scores']= [score1,score2,score3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Αποτελέσματα 1ου ταξινομητή\n",
      "                Σετ  scores\n",
      "0  1ο σετ δεδομένων   0.800\n",
      "1  2ο σετ δεδομένων   0.850\n",
      "2  3ο σετ δεδομένων   0.825\n",
      "\n",
      "Αποτελέσματα 2ου ταξινομητή\n",
      "                Σετ  scores\n",
      "0  1ο σετ δεδομένων   0.825\n",
      "1  2ο σετ δεδομένων   0.825\n",
      "2  3ο σετ δεδομένων   0.875\n",
      "\n",
      "Αποτελέσματα 3ου ταξινομητή\n",
      "                Σετ  scores\n",
      "0  1ο σετ δεδομένων   0.875\n",
      "1  2ο σετ δεδομένων   0.950\n",
      "2  3ο σετ δεδομένων   0.875\n"
     ]
    }
   ],
   "source": [
    "print(\"Αποτελέσματα 1ου ταξινομητή\")\n",
    "print(accuracy_df1)\n",
    "print()\n",
    "print(\"Αποτελέσματα 2ου ταξινομητή\")\n",
    "print(accuracy_df2)\n",
    "print()\n",
    "print(\"Αποτελέσματα 3ου ταξινομητή\")\n",
    "print(accuracy_df3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
