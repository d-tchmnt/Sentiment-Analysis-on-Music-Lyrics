{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dtchmnt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dtchmnt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\dtchmnt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dtchmnt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\dtchmnt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\dtchmnt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# %load ImportsDefinitions.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "#Imports\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import joblib \n",
    "import pickle\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm, tree\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "#accuracy_score(labels_test,pred)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#accuracy_score(labels_test,pred)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "from yellowbrick.datasets import load_hobbies\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.datasets import load_occupancy\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "from yellowbrick.datasets import load_hobbies\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "#Ορισμός συναρτήσεων για να ελέξγχουμε τον καλύτερο vectorizer\n",
    "def tfidf_test_simple(X_train,X_test,y_train,y_test,token_izer):\n",
    "    if (token_izer=='1'):\n",
    "        tfvect= TfidfVectorizer(tokenizer=tokenizer_preproccessor)\n",
    "    elif (token_izer=='2'):\n",
    "        tfvect= TfidfVectorizer(tokenizer=tokenizer_preproccessor_imdb)\n",
    "    else:\n",
    "        tfvect= TfidfVectorizer()\n",
    "    tfidf_train = tfvect.fit_transform(X_train)\n",
    "    tfidf_test = tfvect.transform(X_test)\n",
    "    nb=MultinomialNB()\n",
    "    #train the model and timing it\n",
    "    #kanoume prediction gia to x_test_dtm\n",
    "    \n",
    "    # cross val score/ predict\n",
    "    cvec_score = cross_val_score(nb, tfidf_train, y_train, cv=4 )\n",
    "    feature_names = tfvect.get_feature_names()\n",
    "    print(\"Number of features: {}\".format(len(feature_names)))\n",
    "    print(\"to accuracy tou TFIDF me NB einai {}\".format(cvec_score.mean()))\n",
    "    \n",
    "    visualizer = FreqDistVisualizer(features=feature_names, orient='h')\n",
    "    visualizer.fit(tfidf_train)\n",
    "    visualizer.poof()\n",
    "    \n",
    "    return cvec_score.mean()\n",
    "\n",
    "def countvect_test_simple(X_train,X_test,y_train,y_test,token_izer):\n",
    "    if (token_izer=='1'):\n",
    "        countvect= CountVectorizer(tokenizer=tokenizer_preproccessor)\n",
    "    elif (token_izer=='2'):\n",
    "        countvect= CountVectorizer(tokenizer=tokenizer_preproccessor_imdb)\n",
    "    else:\n",
    "        countvect= CountVectorizer()\n",
    "    #CountVect\n",
    "    countvect.fit(X_train)\n",
    "    #to metatrepoume se dtm sparse matrix\n",
    "    X_train_dtm=countvect.transform(X_train)\n",
    "    X_test_dtm=countvect.transform(X_test)\n",
    "    #Ftiaxnoume Multinomial Naive Bayes modelo\n",
    "    nb=MultinomialNB()\n",
    "    #kanoume prediction gia to x_test_dtm\n",
    "    \n",
    "    # cross val score/ predict\n",
    "    cvec_score = cross_val_score(nb, X_train_dtm, y_train, cv=4 )\n",
    "        \n",
    "          \n",
    "    feature_names = countvect.get_feature_names()\n",
    "    print(\"Number of features: {}\".format(len(feature_names)))\n",
    "    print(\"to accuracy tou CountVectorizer me NB einai: {}\".format(cvec_score.mean()))\n",
    "    \n",
    "    visualizer = FreqDistVisualizer(features=feature_names, orient='h')\n",
    "    visualizer.fit(X_train_dtm)\n",
    "    visualizer.poof()\n",
    "    \n",
    "    return cvec_score.mean()\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "#oρισμός συναρτήσεων για να ελέγχουμε το max_df\n",
    "def countvect_test_maxdf(X_train,X_test,y_train,y_test,token_izer,maxdf):\n",
    "    if (token_izer=='1'):\n",
    "        countvect= CountVectorizer(tokenizer=tokenizer_preproccessor,max_df=maxdf)\n",
    "    elif (token_izer=='2'):\n",
    "        countvect= CountVectorizer(tokenizer=tokenizer_preproccessor_imdb,max_df=maxdf)\n",
    "    else:\n",
    "        countvect= CountVectorizer(max_df=maxdf)\n",
    "    X_train_dtm = countvect.fit_transform(X_train)\n",
    "    X_test_dtm = countvect.transform(X_test)\n",
    "    nb=MultinomialNB()\n",
    "    cvec_score = cross_val_score(nb, X_train_dtm, y_train, cv=4 )\n",
    "   \n",
    "    #kanoume evaluate ta apotelesmata mas me Logistic Regression\n",
    "    feature_names = countvect.get_feature_names()\n",
    "    print(\"Number of features: {}\".format(len(feature_names)))\n",
    "    print(\"To accuracy NB me Max df: {} είναι : {} \".format(maxdf,cvec_score))\n",
    "    return cvec_score.mean()\n",
    "\n",
    "\n",
    "def tfidf_test_maxdf(X_train,X_test,y_train,y_test,token_izer,maxdf):\n",
    "    if (token_izer=='1'):\n",
    "        tfvect= TfidfVectorizer(tokenizer=tokenizer_preproccessor,max_df=maxdf)\n",
    "    elif (token_izer=='2'):\n",
    "        tfvect= TfidfVectorizer(tokenizer=tokenizer_preproccessor_imdb,max_df=maxdf)\n",
    "    else:\n",
    "        tfvect= TfidfVectorizer(max_df=maxdf)\n",
    "    tfidf_train = tfvect.fit_transform(X_train)\n",
    "    tfidf_test = tfvect.transform(X_test)\n",
    "    nb=MultinomialNB()\n",
    "    cvec_score = cross_val_score(nb, tfidf_train, y_train, cv=4 )\n",
    "    \n",
    "    #kanoume evaluate ta apotelesmata mas me Logistic Regression\n",
    "    feature_names = tfvect.get_feature_names()\n",
    "    print(\"Number of features: {}\".format(len(feature_names)))\n",
    "    print(\"To accuracy NB me max df: {} είναι : {} \".format(maxdf,cvec_score))\n",
    "    return cvec_score.mean()\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "def countvect_test_ngrams(X_train,X_test,y_train,y_test,token_izer,ngrams):\n",
    "    if (token_izer=='1'):\n",
    "        countvect= CountVectorizer(tokenizer=tokenizer_preproccessor,ngram_range=(1,ngrams))\n",
    "    elif (token_izer=='2'):\n",
    "        countvect= CountVectorizer(tokenizer=tokenizer_preproccessor_imdb,ngram_range=(1,ngrams))\n",
    "    else:\n",
    "        countvect= CountVectorizer(ngram_range=(1,ngrams))\n",
    "    X_train_dtm = countvect.fit_transform(X_train)\n",
    "    X_test_dtm = countvect.transform(X_test)\n",
    "    nb=MultinomialNB()\n",
    "    cvec_score = cross_val_score(nb, X_train_dtm, y_train, cv=4 )\n",
    "   \n",
    "    #kanoume evaluate ta apotelesmata mas me Logistic Regression\n",
    "    feature_names = countvect.get_feature_names()\n",
    "    print(\"Number of features: {}\".format(len(feature_names)))\n",
    "    print(\"To accuracy NB me {}-ngrams είναι : {} \".format(ngrams,cvec_score))\n",
    "    return cvec_score.mean()\n",
    "\n",
    "def tfidf_test_ngrams(X_train,X_test,y_train,y_test,token_izer,ngrams):\n",
    "    if (token_izer=='1'):\n",
    "        tfvect= TfidfVectorizer(tokenizer=tokenizer_preproccessor,ngram_range=(1,ngrams))\n",
    "    elif (token_izer=='2'):\n",
    "        tfvect= TfidfVectorizer(tokenizer=tokenizer_preproccessor_imdb,ngram_range=(1,ngrams))\n",
    "    else:\n",
    "        tfvect= TfidfVectorizer(ngram_range=(1,ngrams))\n",
    "    tfidf_train = tfvect.fit_transform(X_train)\n",
    "    tfidf_test = tfvect.transform(X_test)\n",
    "    nb=MultinomialNB()\n",
    "    cvec_score = cross_val_score(nb, tfidf_train, y_train, cv=4 )\n",
    "    \n",
    "    #kanoume evaluate ta apotelesmata mas me Logistic Regression\n",
    "    feature_names = tfvect.get_feature_names()\n",
    "    print(\"Number of features: {}\".format(len(feature_names)))\n",
    "    print(\"To accuracy NB me {}-ngrams είναι : {} \".format(ngrams,cvec_score))\n",
    "    return cvec_score.mean()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def countvect_test_maxfeat(X_train,X_test,y_train,y_test,token_izer,maxfeat):  \n",
    "    if (token_izer=='1'):\n",
    "        countvect= CountVectorizer(tokenizer=tokenizer_preproccessor,max_features=maxfeat)\n",
    "    elif (token_izer=='2'):\n",
    "        countvect= CountVectorizer(tokenizer=tokenizer_preproccessor_imdb,max_features=maxfeat)\n",
    "    else:\n",
    "        countvect= CountVectorizer(max_features=maxfeat)\n",
    "    X_train_dtm = countvect.fit_transform(X_train)\n",
    "    X_test_dtm = countvect.transform(X_test)\n",
    "    nb=MultinomialNB()\n",
    "    cvec_score = cross_val_score(nb, X_train_dtm, y_train, cv=4 )\n",
    "   \n",
    "    #kanoume evaluate ta apotelesmata mas me Logistic Regression\n",
    "    feature_names = countvect.get_feature_names()\n",
    "    print(\"Number of features: {}\".format(len(feature_names)))\n",
    "    print(\"To accuracy NB me max features {} είναι : {} \".format(maxfeat,cvec_score))\n",
    "    return cvec_score.mean()\n",
    "\n",
    "def tfidf_test_maxfeat(X_train,X_test,y_train,y_test,token_izer,maxfeat):\n",
    "    if (token_izer=='1'):\n",
    "        tfvect= TfidfVectorizer(tokenizer=tokenizer_preproccessor,max_features=maxfeat)\n",
    "    elif (token_izer=='2'):\n",
    "        tfvect= TfidfVectorizer(tokenizer=tokenizer_preproccessor_imdb,max_features=maxfeat)\n",
    "    else:\n",
    "        tfvect= TfidfVectorizer(max_features=maxfeat)\n",
    "    tfidf_train = tfvect.fit_transform(X_train)\n",
    "    tfidf_test = tfvect.transform(X_test)\n",
    "    nb=MultinomialNB()\n",
    "    cvec_score = cross_val_score(nb, tfidf_train, y_train, cv=4 )\n",
    "    \n",
    "    #kanoume evaluate ta apotelesmata mas me Logistic Regression\n",
    "    feature_names = tfvect.get_feature_names()\n",
    "    print(\"Number of features: {}\".format(len(feature_names)))\n",
    "    print(\"To accuracy NB me max featurues : {} είναι : {} \".format(maxfeat,cvec_score))\n",
    "    return cvec_score.mean()\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "def tokenizer_preproccessor(text):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    #kanoume preprocessing ta dedomena mas\n",
    "    word_tokens=word_tokenize(text.lower()) #kanoume tokenize\n",
    "    #print(\"Έχουμε \" + str(len(word_tokens))+ \" tokens\")\n",
    "    #filtered_word_tokens = [word for word in word_tokens if word not in stop] #svinoume ta stopwords\n",
    "    #print(\"Αφού αφαιρέσαμε τα stopwords, έχουμε τελικά \" + str(len(filtered_word_tokens)) + \" tokens\")\n",
    "    #print(filtered_word_tokens)\n",
    "    filtered_word_tokens = [re.sub(r'[^A-Za-z0-9]+', '', x) for x in word_tokens]  #svinoume ta punctuations\n",
    "    #filtered_word_tokens = [x for x in filtered_word_tokens if x not in ['film','movie']]  #svinoume ta punctuations    \n",
    "    filtered_word_tokens = [word for word in filtered_word_tokens if len(word)>1] #svinoume tis mikres lekseis\n",
    "    #print(\"Αφού αφαιρέσαμε τα σημεία στίξης και τις μικρές λέξεις, έχουμε τελικά \" + str(len(filtered_word_tokens)) + \" tokens\")\n",
    "    #print(filtered_word_tokens)\n",
    "    tagged_filtered_tokens=nltk.pos_tag(filtered_word_tokens) #kanoume pos tag tis lekseis gia syntaktiki analysi\n",
    "    #print(tagged_filtered_tokens)\n",
    "            #ftiaxnoume ta pos tags wste na mpoun san input sto sentisynset\n",
    "    #epeidh ta dedomena tou postag ginontai tuples, ta metatrepoume se lista\n",
    "    newtags=[] #lista me ta nea tags kai idio counter me ta tagged words\n",
    "    for tag in tagged_filtered_tokens:\n",
    "        if tag[1] in set(['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']):\n",
    "            newtags.append('v')\n",
    "        elif tag[1] in set(['JJ', 'JJR', 'JJS']):\n",
    "             newtags.append('a')\n",
    "        elif tag[1] in set(['RB', 'RBR', 'RBS']):\n",
    "             newtags.append('r')\n",
    "        elif tag[1] in set(['NNS', 'NN', 'NNP', 'NNPS']):\n",
    "             newtags.append('n')\n",
    "        else:\n",
    "             newtags.append('a')\n",
    "    \n",
    "    lem_words=[] #edw tha mpoun oi lematized lekseis pou exoume kratisei apo to preprocessing\n",
    "    counter=0 #vazoume ton counter gia na kanoume iterate ta stoixeia tis listas twn tags\n",
    "    for word in tagged_filtered_tokens:    \n",
    "        lem_words.append(wnl.lemmatize(word[0],newtags[counter]))\n",
    "        counter+=1\n",
    "    lem_words = [word for word in lem_words if word not in stop] #svinoume ta stopwords\n",
    "    return lem_words\n",
    "\n",
    "def tokenizer_preproccessor_imdb(text):\n",
    "    stop = nltk.corpus.stopwords.words('english')\n",
    "    stop.append('film')\n",
    "    stop.append('movie')\n",
    "    stop.append('br')\n",
    "    #kanoume preprocessing ta dedomena mas\n",
    "    word_tokens=word_tokenize(text.lower()) #kanoume tokenize\n",
    "    #print(word_tokens)\n",
    "    #print(\"Έχουμε \" + str(len(word_tokens))+ \" tokens\")\n",
    "    #filtered_word_tokens = [word for word in word_tokens if word not in stop] #svinoume ta stopwords\n",
    "    #print(\"Αφού αφαιρέσαμε τα stopwords, έχουμε τελικά \" + str(len(filtered_word_tokens)) + \" tokens\")\n",
    "    #print(filtered_word_tokens)\n",
    "    filtered_word_tokens = [re.sub(r'[^A-Za-z]+', '', x) for x in word_tokens]  #svinoume ta punctuations\n",
    "    filtered_word_tokens = [x for x in filtered_word_tokens if ((x.startswith(\"'\")==0 or x.startswith('-') or x.startswith('.')))]  #svinoume ta punctuations\n",
    "\n",
    "    #filtered_word_tokens = [x for x in filtered_word_tokens if x not in ['film','movie']]  #svinoume ta punctuations    \n",
    "    filtered_word_tokens = [word for word in filtered_word_tokens if len(word)>1] #svinoume tis mikres lekseis\n",
    "    #print(filtered_word_tokens)\n",
    "    #print(\"Αφού αφαιρέσαμε τα σημεία στίξης και τις μικρές λέξεις, έχουμε τελικά \" + str(len(filtered_word_tokens)) + \" tokens\")\n",
    "    #print(filtered_word_tokens)\n",
    "   \n",
    "    tagged_filtered_tokens=nltk.pos_tag(filtered_word_tokens) #kanoume pos tag tis lekseis gia syntaktiki analysi\n",
    "    #print(tagged_filtered_tokens)\n",
    "            #ftiaxnoume ta pos tags wste na mpoun san input sto sentisynset\n",
    "    #epeidh ta dedomena tou postag ginontai tuples, ta metatrepoume se lista\n",
    "    newtags=[] #lista me ta nea tags kai idio counter me ta tagged words\n",
    "    for tag in tagged_filtered_tokens:\n",
    "        if tag[1] in set(['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']):\n",
    "            newtags.append('v')\n",
    "        elif tag[1] in set(['JJ', 'JJR', 'JJS']):\n",
    "             newtags.append('a')\n",
    "        elif tag[1] in set(['RB', 'RBR', 'RBS']):\n",
    "             newtags.append('r')\n",
    "        elif tag[1] in set(['NNS', 'NN', 'NNP', 'NNPS']):\n",
    "             newtags.append('n')\n",
    "        else:\n",
    "             newtags.append('a')\n",
    "    \n",
    "    lem_words=[] #edw tha mpoun oi lematized lekseis pou exoume kratisei apo to preprocessing\n",
    "    counter=0 #vazoume ton counter gia na kanoume iterate ta stoixeia tis listas twn tags\n",
    "    for word in tagged_filtered_tokens:    \n",
    "        lem_words.append(wnl.lemmatize(word[0],newtags[counter]))\n",
    "        counter+=1\n",
    "    lem_words = [word for word in lem_words if word not in stop] #svinoume ta stopwords\n",
    "    #print(lem_words)\n",
    "    \n",
    "    return lem_words\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def SentimentAnalysis_Sentiwordnet(text_samples):\n",
    "    my_sentiments=[] #h lista pou tha periexei tosynaisthitiko score kathe keimenou\n",
    "    my_sentiments_class=[]\n",
    "    #print(stopwords)\n",
    "    stop = set(stopwords.words('english'))\n",
    "    for text in text_samples:\n",
    "        #kanoume preprocessing ta dedomena mas\n",
    "        word_tokens=word_tokenize(text.lower()) #kanoume tokenize\n",
    "        #print(\"Έχουμε \" + str(len(word_tokens))+ \" tokens\")\n",
    "        filtered_word_tokens = [word for word in word_tokens if word not in stop] #svinoume ta stopwords\n",
    "        #print(\"Αφού αφαιρέσαμε τα stopwords, έχουμε τελικά \" + str(len(filtered_word_tokens)) + \" tokens\")\n",
    "        #print(filtered_word_tokens)\n",
    "        filtered_word_tokens = [re.sub(r'[^A-Za-z0-9]+', '', x) for x in filtered_word_tokens]  #svinoume ta punctuations\n",
    "        filtered_word_tokens = [word for word in filtered_word_tokens if len(word)>1] #svinoume tis mikres lekseis\n",
    "        #print(\"Αφού αφαιρέσαμε τα σημεία στίξης και τις μικρές λέξεις, έχουμε τελικά \" + str(len(filtered_word_tokens)) + \" tokens\")\n",
    "        #print(filtered_word_tokens)\n",
    "        tagged_filtered_tokens=nltk.pos_tag(filtered_word_tokens) #kanoume pos tag tis lekseis gia syntaktiki analysi\n",
    "        #print(tagged_filtered_tokens)\n",
    "                #ftiaxnoume ta pos tags wste na mpoun san input sto sentisynset\n",
    "        #epeidh ta dedomena tou postag ginontai tuples, ta metatrepoume se lista\n",
    "        newtags=[] #lista me ta nea tags kai idio counter me ta tagged words\n",
    "        for tag in tagged_filtered_tokens:\n",
    "            if tag[1] in set(['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']):\n",
    "                newtags.append('v')\n",
    "            elif tag[1] in set(['JJ', 'JJR', 'JJS']):\n",
    "                 newtags.append('a')\n",
    "            elif tag[1] in set(['RB', 'RBR', 'RBS']):\n",
    "                 newtags.append('r')\n",
    "            elif tag[1] in set(['NNS', 'NN', 'NNP', 'NNPS']):\n",
    "                 newtags.append('n')\n",
    "            else:\n",
    "                 newtags.append('a')\n",
    "                    \n",
    "                    \n",
    "\n",
    "        lem_words=[] #edw tha mpoun oi lematized lekseis pou exoume kratisei apo to preprocessing\n",
    "        counter=0 #vazoume ton counter gia na kanoume iterate ta stoixeia tis listas twn tags\n",
    "        for word in tagged_filtered_tokens:    \n",
    "            lem_words.append(wnl.lemmatize(word[0],newtags[counter]))\n",
    "            counter+=1\n",
    "           # print (newtags)\n",
    "       # new_words_tags_dict = {'word':'synscore'}\n",
    "        #print(newtags,lem_words)\n",
    "            #print(lem_words)\n",
    "        #ypologizoume to synaisthima kathe leksis , mazi me to POSTAG tis\n",
    "        posscore=0\n",
    "        negscore=0\n",
    "        for i in range(len(lem_words)): \n",
    "            synsets = swn.senti_synsets(lem_words[i],newtags[i])\n",
    "            for synst in synsets: #athroizoume ta thetika kai ta arnhtika score kathe leksis\n",
    "                posscore=posscore+synst.pos_score()\n",
    "                negscore=negscore+synst.neg_score()     \n",
    "        my_sentiments.append(posscore-negscore)\n",
    "        #print(my_sentiments)\n",
    "        if (posscore-negscore)>=0:\n",
    "            my_sentiments_class.append(1)\n",
    "        else:\n",
    "            my_sentiments_class.append(0)\n",
    "    print(len(my_sentiments))\n",
    "    return(my_sentiments_class)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "def SentimentAnalysis_Vader(text_samples):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vader_sentiments=[]\n",
    "    vader_class_sentiments=[]\n",
    "    for text in text_samples:\n",
    "        sum=0\n",
    "        #Kovoume kathe keimeno se protaseis wste na vgalei sentiment polarity o vader, ta opoia athroizoume\n",
    "        sentences=text.split('\\n')   \n",
    "        for sentence in sentences:\n",
    "            sent = analyzer.polarity_scores(sentence)\n",
    "            #print(\"{:-<65} {}\".format(sentence, vs))\n",
    "            sum=sum+sent['compound']\n",
    "        average=sum/len(sentences)\n",
    "        vader_sentiments.append(average)\n",
    "        if (average>=0):\n",
    "            vader_class_sentiments.append(1)       \n",
    "        else:\n",
    "            vader_class_sentiments.append(0)\n",
    "        #print(sentiments)\n",
    "        #print(average)\n",
    "    #Data Examination for Unnotated Dataset\n",
    "    print(\"Έχουμε \" + str(vader_class_sentiments.count(1)) + \" χαρούμενα τραγούδια\")\n",
    "    print(\"Έχουμε \" + str(vader_class_sentiments.count(0)) + \" στενάχωρα τραγούδια\")\n",
    "    return vader_class_sentiments\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def classifier_finder(X_train,X_test,y_train,y_test):\n",
    "    classifiers=[]\n",
    "\n",
    "    scores=[]\n",
    "    model1=LogisticRegression(max_iter=1000)\n",
    "    classifiers.append(model1)\n",
    "    model2=MultinomialNB()\n",
    "    classifiers.append(model2)\n",
    "    model4 = tree.DecisionTreeClassifier()\n",
    "    classifiers.append(model4)\n",
    "    model5 = RandomForestClassifier()\n",
    "    classifiers.append(model5)\n",
    "    model6=LinearSVC(max_iter=2000)\n",
    "    classifiers.append(model6)\n",
    "    \n",
    "    \n",
    "    for clf in classifiers:\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        cvec_score = cross_val_score(clf, X_train, y_train, cv=10 )\n",
    "        \n",
    "        \n",
    "        print(\"Η επιτυχία του  %s είναι:  %s\"%(clf, cvec_score.mean()))\n",
    "        scores.append(cvec_score.mean())\n",
    "# DataFrame Accuracy \n",
    "    scores_df = pd.DataFrame()\n",
    "    scores_df['params']= ['Logistisc Regression','Multinomial Naive Bayes','Decision Tree','Random Forest','Linear SVC']\n",
    "    scores_df['scores']= scores\n",
    "    print(scores_df)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def classifier_finder_music(X_train,y_train):\n",
    "    classifiers=[]\n",
    "    #COUNT VECTORIZER\n",
    "\n",
    "    model1=LogisticRegression()\n",
    "    classifiers.append(model1)\n",
    "    model2 = tree.DecisionTreeClassifier()\n",
    "    classifiers.append(model2)\n",
    "    model3 = RandomForestClassifier()\n",
    "    classifiers.append(model3)\n",
    "    model4=LinearSVC()\n",
    "    classifiers.append(model4)\n",
    "    model5 = KNeighborsClassifier()\n",
    "    classifiers.append(model5)\n",
    "    scorelist=[]\n",
    "\n",
    "    for clf in classifiers:\n",
    "        clf.fit(X_train, y_train)\n",
    "        cvec_score = cross_val_score(clf, X_train, y_train, cv=10)\n",
    "        print(\"Τα αποτελέσματα του Cross-Validation για τον {} είναι:  {} \".format(clf,cvec_score.mean()))                                      \n",
    "        scorelist.append(cvec_score.mean())\n",
    "# DataFrame Accuracy \n",
    "    scores_df = pd.DataFrame()\n",
    "    scores_df['params']= ['Logistisc Regression','Decision Tree','Random Forest','Linear SVC','K-nn']\n",
    "    scores_df['scores']= scorelist\n",
    "    print(scores_df)\n",
    "    \n",
    "\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "#δημιουργούμε συναρτήσεις για να αποθηκεύουμε και να φορτώνουμε  τις λίστες που θα φτιάξουμε με την ανάλυση του vader \n",
    "#και του sentiwordnet\n",
    "def saveList(myList,filename):\n",
    "    np.save(filename,myList)\n",
    "    print(\"Saved successfully!\")\n",
    "    \n",
    "def loadList(filename):\n",
    "    tempNumpyArray=np.load(filename,allow_pickle=True)\n",
    "    return tempNumpyArray.tolist()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.21.2 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.21.2 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# # φορτώνουμε τους classifiers που φτιάξαμε\n",
    "imdb_clf = joblib.load('imdb_clf.pkl')\n",
    "sentiwordnet_clf = joblib.load('sentiwordnet_clf.pkl')\n",
    "vader_clf = joblib.load('vader_clf.pkl')\n",
    "#φορτωνουμε τους vectorizers που φτιάξαμε, ώστε να κάνουμε transform τα X_test με βαση αυτα τα vects\n",
    "vader_cvec = joblib.load('vader_cvec.pkl')\n",
    "sentiwordnet_cvec = joblib.load('sentiwordnet_cvec.pkl')\n",
    "imdb_tvec=joblib.load('imdb_tvec.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#φορτώνουμε τα σετ ε΄λέγχου \n",
    "X_test_sentiwordnet=loadList(\"X_test_sentiwordnet.npy\")\n",
    "y_test_sentiwordnet=loadList(\"y_test_sentiwordnet.npy\")\n",
    "X_test_imdb=loadList(\"X_test_imdb.npy\")\n",
    "y_test_imdb=loadList(\"y_test_imdb.npy\")\n",
    "X_test_vader=loadList(\"X_test_vader.npy\")\n",
    "y_test_vader=loadList(\"y_test_vader.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vader_by_vader_dtm=loadList('X_test_vader_dtm.npy')\n",
    "X_sentiwordnet_by_sentiwordnet_dtm=loadList('X_test_sentiwordnet_dtm.npy')\n",
    "X_imdb_by_imdb_dtm=loadList('X_test_imdb_dtm.npy')\n",
    "#kanoume transform ta ypoloipa dianysmata, wste na mporoume na kanoume predictions gia to ekastote X_test\n",
    "#gia ton vect tou vader\n",
    "X_sentiwordnet_by_vader_dtm = vader_cvec.transform(X_test_sentiwordnet)\n",
    "X_imdb_by_vader_dtm = vader_cvec.transform(X_test_imdb)\n",
    "#gia ton vect tou sentiwordnet\n",
    "X_vader_by_sentiwordnet_dtm = sentiwordnet_cvec.transform(X_test_vader)\n",
    "X_imdb_by_sentiwordnet_dtm = sentiwordnet_cvec.transform(X_test_imdb)\n",
    "#gia ton vect tou imdb\n",
    "X_vader_by_imdb_dtm = imdb_tvec.transform(X_test_vader)\n",
    "X_sentiwordnet_by_imdb_dtm = imdb_tvec.transform(X_test_sentiwordnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Η επιτυχία του ταξινομητή του Vader για το σετ δεδομένων Vader είναι 0.8638334778837814\n",
      "Η επιτυχία του ταξινομητή του Vader για το σετ δεδομένων SentiWordNet είναι 0.7676496097137901\n",
      "Η επιτυχία του ταξινομητή του Vader για το σετ δεδομένων Imdb είναι 0.6371\n",
      "Πολλαπλασιάζουμε τα σκορ που βρήκαμε\n",
      "Μέσος Όρος:  0.7561943625325238\n",
      "Μέσος Όρος μόνο για στίχους:  0.8157415437987858\n",
      "\n",
      "Η επιτυχία του ταξινομητή του SentiWordNet για το σετ δεδομένων Vader είναι 0.7393755420641804\n",
      "Η επιτυχία του ταξινομητή του SentiWordNet για το σετ δεδομένων SentiWordNet είναι 0.9300086730268864\n",
      "Η επιτυχία του ταξινομητή του SentiWordNet για το σετ δεδομένων Imdb είναι 0.5945\n",
      "Πολλαπλασιάζουμε τα σκορ που βρήκαμε\n",
      "Μέσος Όρος:  0.7546280716970223\n",
      "Μέσος Όρος μόνο για στίχους:  0.8346921075455334\n",
      "\n",
      "Η επιτυχία του ταξινομητή του Imdb για το σετ δεδομένων Vader είναι 0.6427580225498699\n",
      "Η επιτυχία του ταξινομητή του Imdb για το σετ δεδομένων SentiWordNet είναι 0.6480485689505637\n",
      "Η επιτυχία του ταξινομητή του Imdb για το σετ δεδομένων Imdb είναι 0.908\n",
      "Πολλαπλασιάζουμε τα σκορ που βρήκαμε\n",
      "Μέσος Όρος:  0.7329355305001446\n",
      "Μέσος Όρος μόνο για στίχους:  0.6454032957502168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#kanoume provlepsh gia to kathe dataset me vash ton classifier tou Vader\n",
    "#ta onomata twn metavlitwn pou apothikevoume, anaferontai prwta sto dataset gia to opoio dokimazoume, kai meta gia \n",
    "#ton classifier. Ara to vader_score_vader anaferetai ston vader classifier pou tha prospathisei na kanei predictions gia to vader\n",
    "#dataset. Antistoixa, to sentiwordnet_score_vader, shmainei pws o vader classifier tha prospathisei na kanei predictions gia to\n",
    "#set dedomenwn apo to sentiwordnet\n",
    "ypred_for_vader = vader_clf.predict(X_vader_by_vader_dtm)\n",
    "vader_score_vader=accuracy_score(y_test_vader, ypred_for_vader)\n",
    "\n",
    "ypred_for_sentiwordnet =vader_clf.predict(X_sentiwordnet_by_vader_dtm)\n",
    "sentiwordnet_score_vader = accuracy_score(y_test_sentiwordnet, ypred_for_sentiwordnet)\n",
    "\n",
    "ypred_for_imdb =vader_clf.predict(X_imdb_by_vader_dtm)\n",
    "imdb_score_vader=accuracy_score(y_test_imdb,ypred_for_imdb)\n",
    "print(\"Η επιτυχία του ταξινομητή του Vader για το σετ δεδομένων Vader είναι {}\".format(vader_score_vader))\n",
    "print(\"Η επιτυχία του ταξινομητή του Vader για το σετ δεδομένων SentiWordNet είναι {}\".format(sentiwordnet_score_vader))\n",
    "print(\"Η επιτυχία του ταξινομητή του Vader για το σετ δεδομένων Imdb είναι {}\".format(imdb_score_vader))\n",
    "print(\"Πολλαπλασιάζουμε τα σκορ που βρήκαμε\")\n",
    "print(\"Μέσος Όρος: \",(vader_score_vader+sentiwordnet_score_vader+imdb_score_vader)/3)\n",
    "print(\"Μέσος Όρος μόνο για στίχους: \",(vader_score_vader+sentiwordnet_score_vader)/2)\n",
    "print()\n",
    "#kanoume provlepsh gia to kathe dataset me vash ton classifier tou SentiWordNet\n",
    "ypred_for_vader = sentiwordnet_clf.predict(X_vader_by_sentiwordnet_dtm)\n",
    "vader_score_sentiwordnet=accuracy_score(y_test_vader, ypred_for_vader)\n",
    "\n",
    "ypred_for_sentiwordnet =sentiwordnet_clf.predict(X_sentiwordnet_by_sentiwordnet_dtm)\n",
    "sentiwordnet_score_sentiwordnet = accuracy_score(y_test_sentiwordnet, ypred_for_sentiwordnet)\n",
    "\n",
    "ypred_for_imdb =sentiwordnet_clf.predict(X_imdb_by_sentiwordnet_dtm)\n",
    "imdb_score_sentiwordnet=accuracy_score(y_test_imdb,ypred_for_imdb)\n",
    "print(\"Η επιτυχία του ταξινομητή του SentiWordNet για το σετ δεδομένων Vader είναι {}\".format(vader_score_sentiwordnet))\n",
    "print(\"Η επιτυχία του ταξινομητή του SentiWordNet για το σετ δεδομένων SentiWordNet είναι {}\".format(sentiwordnet_score_sentiwordnet))\n",
    "print(\"Η επιτυχία του ταξινομητή του SentiWordNet για το σετ δεδομένων Imdb είναι {}\".format(imdb_score_sentiwordnet))\n",
    "print(\"Πολλαπλασιάζουμε τα σκορ που βρήκαμε\")\n",
    "print(\"Μέσος Όρος: \",(vader_score_sentiwordnet+sentiwordnet_score_sentiwordnet+imdb_score_sentiwordnet)/3)\n",
    "print(\"Μέσος Όρος μόνο για στίχους: \",(vader_score_sentiwordnet+sentiwordnet_score_sentiwordnet)/2)\n",
    "\n",
    "print()\n",
    "\n",
    "#kanoume provlepsh gia to kathe dataset me vash ton classifier tou imdb\n",
    "ypred_for_vader = imdb_clf.predict(X_vader_by_imdb_dtm)\n",
    "vader_score_imdb=accuracy_score(y_test_vader, ypred_for_vader)\n",
    "\n",
    "ypred_for_sentiwordnet =imdb_clf.predict(X_sentiwordnet_by_imdb_dtm)\n",
    "sentiwordnet_score_imdb = accuracy_score(y_test_sentiwordnet, ypred_for_sentiwordnet)\n",
    "\n",
    "ypred_for_imdb =imdb_clf.predict(X_imdb_by_imdb_dtm)\n",
    "imdb_score_imdb=accuracy_score(y_test_imdb,ypred_for_imdb)\n",
    "print(\"Η επιτυχία του ταξινομητή του Imdb για το σετ δεδομένων Vader είναι {}\".format(vader_score_imdb))\n",
    "print(\"Η επιτυχία του ταξινομητή του Imdb για το σετ δεδομένων SentiWordNet είναι {}\".format(sentiwordnet_score_imdb))\n",
    "print(\"Η επιτυχία του ταξινομητή του Imdb για το σετ δεδομένων Imdb είναι {}\".format(imdb_score_imdb))\n",
    "print(\"Πολλαπλασιάζουμε τα σκορ που βρήκαμε\")\n",
    "print(\"Μέσος Όρος: \",(vader_score_imdb+sentiwordnet_score_imdb+imdb_score_imdb)/3)\n",
    "print(\"Μέσος Όρος μόνο για στίχους: \",(vader_score_imdb+sentiwordnet_score_imdb)/2)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
