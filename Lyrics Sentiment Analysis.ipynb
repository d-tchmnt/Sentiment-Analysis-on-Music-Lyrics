{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "path='songdata.csv'\n",
    "data = pd.read_csv(path)\n",
    "all_texts=data.text\n",
    "#gia to peirama mas tha xrisimopoisoume tin akollouthi timi documents\n",
    "text_samples=data.text.head(5000)\n",
    "text_sample1=data.text[0]\n",
    "#print(text_samples)\n",
    "word_counter=0\n",
    "#metrame poses lekseis exoume ahtroistika sto text_samples\n",
    "print(\"done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dtchmnt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dtchmnt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\dtchmnt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dtchmnt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\dtchmnt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\dtchmnt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESSING KAI YLOPOIISI DIKHS MAS SYNAISTHIMATIKHS ANALYSIS\n",
    "#kanoume tokenize tis lekseis\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "my_sentiments=[] #h lista pou tha periexei tosynaisthitiko score kathe keimenou\n",
    "my_sentiments_class=[]\n",
    "#print(stopwords)\n",
    "print(len(text_samples))\n",
    "\n",
    "for text in text_samples:\n",
    "    #kanoume preprocessing ta dedomena mas\n",
    "    word_tokens=word_tokenize(text.lower()) #kanoume tokenize\n",
    "    filtered_word_tokens = [word for word in word_tokens if word not in stopwords.words('english')] #svinoume ta stopwords\n",
    "    tagged_filtered_tokens=nltk.pos_tag(filtered_word_tokens) #kanoume pos tag tis lekseis gia syntaktiki analysi\n",
    "    \n",
    "    \n",
    "        #ftiaxnoume ta pos tags wste na mpoun san input sto sentisynset\n",
    "    #epeidh ta dedomena tou postag ginontai tuples, ta metatrepoume se lista\n",
    "    newtags=[] #lista me ta nea tags kai idio counter me ta tagged words\n",
    "    for tag in tagged_filtered_tokens:\n",
    "        if tag[1] in set(['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']):\n",
    "            newtags.append('v')\n",
    "        elif tag[1] in set(['JJ', 'JJR', 'JJS']):\n",
    "             newtags.append('a')\n",
    "        elif tag[1] in set(['RB', 'RBR', 'RBS']):\n",
    "             newtags.append('r')\n",
    "        elif tag[1] in set(['NNS', 'NN', 'NNP', 'NNPS']):\n",
    "             newtags.append('n')\n",
    "        else:\n",
    "             newtags.append('a')\n",
    "\n",
    "\n",
    "    lem_words=[] #edw tha mpoun oi lematized lekseis pou exoume kratisei apo to preprocessing\n",
    "    counter=0 #vazoume ton counter gia na kanoume iterate ta stoixeia tis listas twn tags\n",
    "    for word in tagged_filtered_tokens:    \n",
    "        lem_words.append(wnl.lemmatize(word[0],newtags[counter]))\n",
    "        counter+=1\n",
    "       # print (newtags)\n",
    "   # new_words_tags_dict = {'word':'synscore'}\n",
    "\n",
    "    #print(lem_words)\n",
    "    #ypologizoume to synaisthima kathe leksis , mazi me to POSTAG tis\n",
    "    for i in range(len(lem_words)): \n",
    "        synsets = swn.senti_synsets(lem_words[i],newtags[i])\n",
    "        posscore=0\n",
    "        negscore=0\n",
    "        for synst in synsets: #athroizoume ta thetika kai ta arnhtika score kathe leksis\n",
    "            posscore=posscore+synst.pos_score()\n",
    "            negscore=negscore+synst.neg_score()     \n",
    "    my_sentiments.append(posscore-negscore)\n",
    "    if (posscore-negscore)>=0:\n",
    "        my_sentiments_class.append(1)\n",
    "    else:\n",
    "        my_sentiments_class.append(0)\n",
    "print(len(my_sentiments))\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "def tokenizer_preproccessor(text):\n",
    "    #kanoume preprocessing ta dedomena mas\n",
    "    tokens=word_tokenize(text.lower()) #kanoume tokenize\n",
    "    POS_nltk_wordtags=nltk.pos_tag(tokens) #kanoume pos tag tis lekseis gia syntaktiki analysi\n",
    "   # print(POS_nltk_wordtags[:1])\n",
    "    #print(\"exoume arxika \" + str(len(POS_nltk_wordtags))+ \" POSTAGS\")\n",
    "        #ftiaxnoume ta pos tags wste na mpoun san input sto sentisynset\n",
    "    #epeidh ta dedomena tou postag ginontai tuples, ta metatrepoume se lista\n",
    "    \n",
    "    \"\"\"\"\"valame: CC: syndesmoi,CD: arithmos olografws,DT:Determiner this book my sister,IN\tPreposition or subordinating conjunction(in,at)\"\n",
    "        JJ\tAdjective,JJR\tAdjective, comparative,JJS\tAdjective, superlative,MD\tModal(can,could,may)\n",
    "        NN\tNoun, singular or mass NNS\tNoun, plural,PDT\tPredeterminer POS\tPossessive ending PRP\tPersonal pronoun\n",
    "       PRP$\tPossessive pronoun RB\tAdverb RBR\tAdverb, comparative RBS\tAdverb, superlative,RP\tParticle(i tidy UP the room)\n",
    "       SYM\tSymbol,UH\tInterjection(epifwnhma)\n",
    "       VB\tVerb, base form VBD\tVerb, past tense VBG\tVerb, gerund or present participle VBN\tVerb, past participle\n",
    "    VBP\tVerb, non-3rd person singular present VBZ\tVerb, 3rd person singular present\n",
    "    WDT\tWh-determiner WP\tWh-pronoun WP$\tPossessive wh-pronoun WRB\tWh-adverb\n",
    "       \n",
    "         kopsame : FW\tForeign word,EX\tExistential there,LS\tList item marker,NNP\tProper noun, singular,\n",
    "         Proper noun, plural\n",
    "    \n",
    "    newtags=[] #lista me tis lekseis kai to POSTAG tous\n",
    "    for tag in POS_words_tags:\n",
    "        if tag[1] in set(['CC','CD','DT','EX','IN','JJ','JJR','JJS','MD','NN','NNS','PDT','POS','PRP','PRP$','RB','RBR','RBS','RP','SYM','TO','UH','VB','VBD','VBG','VBN','VBP','VBZ','WDT','WP','WP$','WRB']):\n",
    "            newtags.append(tag)\n",
    "       \n",
    "        else:\n",
    "             print(tag)\n",
    "    \"\"\"\n",
    "    wn_tags=[]\n",
    "    for pos in POS_nltk_wordtags:\n",
    "        if pos[1] in set(['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']):\n",
    "            wn_tags.append('v')\n",
    "        elif pos[1] in set(['JJ', 'JJR', 'JJS']):\n",
    "             wn_tags.append('a')\n",
    "        elif pos[1] in set(['RB', 'RBR', 'RBS']):\n",
    "             wn_tags.append('r')\n",
    "        elif pos[1] in set(['NNS', 'NN', 'NNP', 'NNPS']):\n",
    "             wn_tags.append('n')\n",
    "        else:\n",
    "             wn_tags.append('a')\n",
    "    \n",
    "    \n",
    "    #print(wn_tags)\n",
    "   \n",
    "     #edw tha mpoun oi lematized lekseis \n",
    "    lem_words=[] #edw tha mpoun oi lematized lekseis pou exoume kratisei apo to preprocessing\n",
    "    counter=0 #vazoume ton counter gia na kanoume iterate ta stoixeia tis listas twn tags\n",
    "    for word in POS_nltk_wordtags:    \n",
    "        lem_words.append(wnl.lemmatize(word[0],wn_tags[counter]))\n",
    "        counter+=1\n",
    "    tokens = [word for word in lem_words if word not in stopwords.words('english')] #svinoume ta stopwords\n",
    "    tokens = [t for t in tokens if len(t) > 1] # remove short words, they're probably not useful\n",
    "    #print(\"kratisame \" + str(len(tokens)) + \" tags telika\")\n",
    "    #print(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for text in text_samples:\n",
    "    tokenizer_preproccessor(text)\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#sentiment analysis with vader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "vader_sentiments=[]\n",
    "vader_class_sentiments=[]\n",
    "for text in text_samples:\n",
    "    sum=0\n",
    "    #Kovoume kathe keimeno se protaseis wste na vgalei sentiment polarity o vader, ta opoia athroizoume\n",
    "    sentences=text.split('\\n')   \n",
    "    for sentence in sentences:\n",
    "        sent = analyzer.polarity_scores(sentence)\n",
    "        #print(\"{:-<65} {}\".format(sentence, vs))\n",
    "        sum=sum+sent['compound']\n",
    "    average=sum/len(sentences)\n",
    "    vader_sentiments.append(average)\n",
    "    if (average>=0):\n",
    "        vader_class_sentiments.append(1)       \n",
    "    else:\n",
    "        vader_class_sentiments.append(0)\n",
    "    #print(sentiments)\n",
    "    #print(average)\n",
    "print(len(vader_sentiments))\n",
    "print(len(vader_class_sentiments))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_sentiments_class' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5e30db1e9c3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvader_class_sentiments\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmy_sentiments_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvader_class_sentiments\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmy_sentiments_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'my_sentiments_class' is not defined"
     ]
    }
   ],
   "source": [
    "#SYGKRISI TIS ANALYSIS MAS ME TOU VADER\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(accuracy_score(vader_class_sentiments,my_sentiments_class))\n",
    "confusion_matrix(vader_class_sentiments,my_sentiments_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "#accuracy_score(labels_test,pred)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_samples, vader_class_sentiments, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31.2 ms\n",
      "Number of features: 16809\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c5549036119f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of features: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"to accuracy NB me tokenizer kai unigram einai \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_predictions_NB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "#COUNT VECTORIZER ME UNIGRAM\n",
    "vect = CountVectorizer(tokenizer=tokenizer_preproccessor,ngram_range=(1,1))\n",
    "#mathainoume to leksilogio tou x_train\n",
    "vect.fit(X_train)\n",
    "#to metatrepoume se dtm sparse matrix\n",
    "X_train_dtm=vect.transform(X_train)\n",
    "X_test_dtm=vect.transform(X_test)\n",
    "#Ftiaxnoume Multinomial Naive Bayes modelo\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb=MultinomialNB()\n",
    "#train the model and timing it\n",
    "%time nb.fit(X_train_dtm,y_train)\n",
    "#kanoume prediction gia to x_test_dtm\n",
    "y_predictions_NB=nb.predict(X_test_dtm)\n",
    "#calculate the accuracy of class prediction\n",
    "metrics.confusion_matrix(y_test,y_predictions_NB)\n",
    "feature_names = vect.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"to accuracy NB me tokenizer kai unigram einai \" + str(accuracy_score(y_test,y_predictions_NB)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "to accuracy NB me tokenizer kai bigram einai 0.74\n"
     ]
    }
   ],
   "source": [
    "#COUNT VECTORIZER ME bigram\n",
    "vect = CountVectorizer(tokenizer=tokenizer_preproccessor,ngram_range=(2,2))\n",
    "#mathainoume to leksilogio tou x_train\n",
    "vect.fit(X_train)\n",
    "#to metatrepoume se dtm sparse matrix\n",
    "X_train_dtm=vect.transform(X_train)\n",
    "X_test_dtm=vect.transform(X_test)\n",
    "#Ftiaxnoume Multinomial Naive Bayes modelo\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb=MultinomialNB()\n",
    "#train the model and timing it\n",
    "%time nb.fit(X_train_dtm,y_train)\n",
    "#kanoume prediction gia to x_test_dtm\n",
    "y_predictions_NB=nb.predict(X_test_dtm)\n",
    "#calculate the accuracy of class prediction\n",
    "metrics.confusion_matrix(y_test,y_predictions_NB)\n",
    "feature_names = vect.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"to accuracy NB me tokenizer kai bigram einai \" + str(accuracy_score(y_test,y_predictions_NB)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.6 ms\n",
      "to accuracy NB me tokenizer kai uni+bigram einai 0.76\n"
     ]
    }
   ],
   "source": [
    "#COUNT VECTORIZER ME UNIGRAM kai bigram\n",
    "vect = CountVectorizer(tokenizer=tokenizer_preproccessor,ngram_range=(1,2))\n",
    "#mathainoume to leksilogio tou x_train\n",
    "vect.fit(X_train)\n",
    "#to metatrepoume se dtm sparse matrix\n",
    "X_train_dtm=vect.transform(X_train)\n",
    "X_test_dtm=vect.transform(X_test)\n",
    "#Ftiaxnoume Multinomial Naive Bayes modelo\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb=MultinomialNB()\n",
    "#train the model and timing it\n",
    "%time nb.fit(X_train_dtm,y_train)\n",
    "#kanoume prediction gia to x_test_dtm\n",
    "y_predictions_NB=nb.predict(X_test_dtm)\n",
    "#calculate the accuracy of class prediction\n",
    "metrics.confusion_matrix(y_test,y_predictions_NB)\n",
    "feature_names = vect.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"to accuracy NB me tokenizer kai uni+bigram einai \" + str(accuracy_score(y_test,y_predictions_NB)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.98 ms\n",
      "Number of features: 117732\n",
      "to accuracy NB me tokenizer kai uni+bigram+trigram einai 0.76\n"
     ]
    }
   ],
   "source": [
    "#COUNT VECTORIZER ME TRIGRAM\n",
    "vect = CountVectorizer(tokenizer=tokenizer_preproccessor,ngram_range=(1,3))\n",
    "#mathainoume to leksilogio tou x_train\n",
    "vect.fit(X_train)\n",
    "#to metatrepoume se dtm sparse matrix\n",
    "X_train_dtm=vect.transform(X_train)\n",
    "X_test_dtm=vect.transform(X_test)\n",
    "#Ftiaxnoume Multinomial Naive Bayes modelo\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb=MultinomialNB()\n",
    "#train the model and timing it\n",
    "%time nb.fit(X_train_dtm,y_train)\n",
    "#kanoume prediction gia to x_test_dtm\n",
    "y_predictions_NB=nb.predict(X_test_dtm)\n",
    "#calculate the accuracy of class prediction\n",
    "metrics.confusion_matrix(y_test,y_predictions_NB)\n",
    "feature_names = vect.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"to accuracy NB me tokenizer kai uni+bigram+trigram einai \" + str(accuracy_score(y_test,y_predictions_NB)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 117732\n",
      "0.78\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 15,  32],\n",
       "       [ 12, 141]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dokimazoume me logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#arxikopoioume to LR modelo\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_dtm,y_train)\n",
    "\n",
    "#\n",
    "y_predictions_LR=logreg.predict(X_test_dtm)\n",
    "#kanoume evaluate ta apotelesmata mas me Logistic Regression\n",
    "feature_names = vect.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(metrics.accuracy_score(y_test,y_predictions_LR))\n",
    "metrics.confusion_matrix(y_test,y_predictions_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"vect2 = CountVectorizer()\\nvect2.fit(X_train)\\nX2_train_dtm=vect2.transform(X_train)\\nX2_test_dtm=vect2.transform(X_test)\\n#train the model and timing it\\nnb=MultinomialNB()\\n%time nb.fit(X2_train_dtm,y_train)\\n#kanoume prediction gia to x_test_dtm\\ny2_predictions_NB=nb.predict(X2_test_dtm)\\n#calculate the accuracy of class prediction\\nmetrics.confusion_matrix(y_test,y2_predictions_NB)\\nprint(\"to accuracy NB me tokenizer kai preprocessor einai \" + str(accuracy_score(y_test,y_predictions_NB)))'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#COUNT VECTORIZER XWRIS TOKENIZER\n",
    "\"\"\"\"vect2 = CountVectorizer()\n",
    "vect2.fit(X_train)\n",
    "X2_train_dtm=vect2.transform(X_train)\n",
    "X2_test_dtm=vect2.transform(X_test)\n",
    "#train the model and timing it\n",
    "nb=MultinomialNB()\n",
    "%time nb.fit(X2_train_dtm,y_train)\n",
    "#kanoume prediction gia to x_test_dtm\n",
    "y2_predictions_NB=nb.predict(X2_test_dtm)\n",
    "#calculate the accuracy of class prediction\n",
    "metrics.confusion_matrix(y_test,y2_predictions_NB)\n",
    "print(\"to accuracy NB me tokenizer kai preprocessor einai \" + str(accuracy_score(y_test,y_predictions_NB)))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.98 ms\n",
      "Number of features: 104820\n",
      "to accuracy NB me tokenizer kai uni+bigram einai 0.76\n"
     ]
    }
   ],
   "source": [
    "#dokimazoume me tfidf kai NB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "vect = TfidfVectorizer(lowercase=True, analyzer='word',stop_words= 'english',ngram_range=(1, 3))\n",
    "tfidf_train = vect.fit_transform(X_train)\n",
    "tfidf_test = vect.transform(X_test)\n",
    "nb=MultinomialNB()\n",
    "#train the model and timing it\n",
    "%time nb.fit(X_train_dtm,y_train)\n",
    "#kanoume prediction gia to x_test_dtm\n",
    "y_predictions_NB=nb.predict(X_test_dtm)\n",
    "#calculate the accuracy of class prediction\n",
    "metrics.confusion_matrix(y_test,y_predictions_NB)\n",
    "feature_names = vect.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"to accuracy NB me tokenizer kai uni+bigram einai \" + str(accuracy_score(y_test,y_predictions_NB)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 104820\n",
      "0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dtchmnt\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  3,  44],\n",
       "       [  0, 153]], dtype=int64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TFIDF KAI LOGREG\n",
    "logreg = LogisticRegression(C=30, dual=True)\n",
    "logreg.fit(tfidf_train,y_train)\n",
    "y_predictions_LR2=logreg.predict(tfidf_test)\n",
    "feature_names = vect.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(metrics.accuracy_score(y_test,y_predictions_LR2))\n",
    "metrics.confusion_matrix(y_test,y_predictions_LR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
